{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500989c7-8c2c-44e5-a72e-fea419b4d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Cell 1:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
      "\n",
      "import os\n",
      "import zipfile\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from textblob import TextBlob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
      "from statsmodels.tsa.arima.model import ARIMA\n",
      "\n",
      "import time\n",
      "\n",
      "notebook_start_time = time.time()\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 2:\n",
      "\n",
      "# Unzip the file directly in the current directory\n",
      "with zipfile.ZipFile('stock-tweet-and-price.zip', 'r') as zip_ref:\n",
      "    zip_ref.extractall()\n",
      "\n",
      "# Load the tweet dataset\n",
      "tweets_df = pd.read_csv('stock-tweet-and-price/stocktweet/stocktweet.csv')\n",
      "\n",
      "# Display the first few rows of the dataframe\n",
      "tweets_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 3:\n",
      "\n",
      "os.listdir('stock-tweet-and-price')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 4:\n",
      "\n",
      "# List files in the directory and convert to a list\n",
      "stock_files = os.listdir('stock-tweet-and-price/stockprice')\n",
      "\n",
      "# Display the list horizontally\n",
      "print(stock_files)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 5:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "companies = ['AAPL', 'AMZN', 'GOOG', 'MSFT', 'TSLA']\n",
      "stock_data = {}\n",
      "\n",
      "for company in companies:\n",
      "    company_csv_path = os.path.join('stock-tweet-and-price/stockprice', f'{company}.csv')\n",
      "    df_name = f'{company.lower()}_df'\n",
      "    stock_data[company] = pd.read_csv(company_csv_path)\n",
      "    \n",
      "    # Convert `Date` column to datetime\n",
      "    stock_data[company]['Date'] = pd.to_datetime(stock_data[company]['Date'])\n",
      "    \n",
      "    # Filter out rows with dates in 2019 in the stock data\n",
      "    stock_data[company] = stock_data[company][stock_data[company]['Date'].dt.year != 2019]\n",
      "    \n",
      "    # Create a variable with a name like goog_df\n",
      "    globals()[df_name] = stock_data[company]\n",
      "    \n",
      "    print(f\"First few rows of the {company} dataset:\")\n",
      "    display(stock_data[company].head())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 6:\n",
      "\n",
      "\n",
      "\n",
      "# Function to calculate sentiment polarity\n",
      "def get_sentiment(tweet):\n",
      "    analysis = TextBlob(tweet)\n",
      "    return analysis.sentiment.polarity\n",
      "\n",
      "# Apply the sentiment analysis function to the tweet text\n",
      "tweets_df['sentiment'] = tweets_df['tweet'].apply(get_sentiment)\n",
      "\n",
      "# Update the tweets dataset with sentiment scores\n",
      "print(\"First few rows of the tweets dataset with sentiment scores:\")\n",
      "tweets_df.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 7:\n",
      "\n",
      "tweets_df['sentiment'].describe()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 8:\n",
      "\n",
      "\n",
      "# Ensure the images directory exists\n",
      "images_dir = 'images'\n",
      "os.makedirs(images_dir, exist_ok=True)\n",
      "\n",
      "# Plot the distribution of the sentiment scores\n",
      "plt.figure(figsize=(10, 6))\n",
      "sns.displot(tweets_df['sentiment'], kde=True)\n",
      "plt.title('Distribution of Sentiment Scores')\n",
      "plt.xlabel('Sentiment Score')\n",
      "plt.ylabel('Frequency')\n",
      "\n",
      "# Save the plot to the images folder\n",
      "image_path = os.path.join(images_dir, 'sentiment_distribution.png')\n",
      "plt.savefig(image_path)\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "print(f\"Plot saved as {image_path}\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 9:\n",
      "\n",
      "# Convert date column to datetime\n",
      "tweets_df['date'] = pd.to_datetime(tweets_df['date'], format='%d/%m/%Y')\n",
      "\n",
      "# Aggregate sentiment scores by date\n",
      "daily_sentiment = tweets_df.groupby('date')['sentiment'].mean().reset_index()\n",
      "\n",
      "# Convert `date` column in `daily_sentiment` to datetime\n",
      "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
      "\n",
      "# Merge the sentiment scores with the stock price data for each company\n",
      "for company in companies:\n",
      "    df = stock_data[company]\n",
      "    df['Date'] = pd.to_datetime(df['Date'])\n",
      "    \n",
      "    # Merge sentiment scores\n",
      "    df = df.merge(daily_sentiment, left_on='Date', right_on='date', how='left')\n",
      "    df = df.drop(columns=['date'])\n",
      "    \n",
      "    stock_data[company] = df\n",
      "\n",
      "    # Display the merged dataset\n",
      "    print(f\"First few rows of the merged {company} dataset:\")\n",
      "    display(df.head())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 10:\n",
      "\n",
      "\n",
      "\n",
      "# Function to create lag features\n",
      "def create_lag_features(df, lags, target_col):\n",
      "    for lag in lags:\n",
      "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
      "    return df\n",
      "\n",
      "# Define the lags and the target column\n",
      "lags = [1, 3, 7]\n",
      "target_col = 'Close'\n",
      "\n",
      "# Prepare the data for each company\n",
      "prepared_stock_data = {}\n",
      "for company in companies:\n",
      "    df = stock_data[company]\n",
      "    \n",
      "    # Create lag features for the Close price and sentiment\n",
      "    df = create_lag_features(df, lags, target_col)\n",
      "    df = create_lag_features(df, lags, 'sentiment')\n",
      "    \n",
      "    # Drop rows with NaN values created by the lag features\n",
      "    df = df.dropna()\n",
      "    \n",
      "    # Set Date as index\n",
      "    df = df.set_index('Date')\n",
      "    \n",
      "    # Split the data into training and testing sets\n",
      "    X = df.drop(columns=['Close'])\n",
      "    y = df['Close']\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
      "    \n",
      "    # Store the prepared data\n",
      "    prepared_stock_data[company] = (X_train, X_test, y_train, y_test)\n",
      "\n",
      "    print(f\"Prepared data for {company}:\")\n",
      "    print(\"X_train:\")\n",
      "    display(X_train.head())\n",
      "    print(\"y_train:\")\n",
      "    display(y_train.head())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 11:\n",
      "\n",
      "\n",
      "\n",
      "# Suppress warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "# Create the images directory if it doesn't exist\n",
      "images_dir = 'images'\n",
      "if not os.path.exists(images_dir):\n",
      "    os.makedirs(images_dir)\n",
      "\n",
      "# Function to train and evaluate ARIMA model\n",
      "def train_arima(y_train, y_test, steps=7):\n",
      "    # Ensure the index is of DatetimeIndex type and set frequency\n",
      "    y_train.index = pd.DatetimeIndex(y_train.index)\n",
      "    y_train.index.freq = y_train.index.inferred_freq\n",
      "    y_test.index = pd.DatetimeIndex(y_test.index)\n",
      "    y_test.index.freq = y_test.index.inferred_freq\n",
      "    \n",
      "    model = ARIMA(y_train, order=(5, 1, 0))\n",
      "    model_fit = model.fit()\n",
      "    \n",
      "    # Forecast\n",
      "    forecast = model_fit.forecast(steps=steps)\n",
      "    \n",
      "    # Evaluate the model\n",
      "    mse = mean_squared_error(y_test[:steps], forecast)\n",
      "    print(f'Mean Squared Error: {mse}')\n",
      "    \n",
      "    return forecast, mse\n",
      "\n",
      "# Train and evaluate ARIMA model for each company\n",
      "forecast_horizon = 7  # You can extend this as needed\n",
      "img_number = 1  # Initialize image numbering\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    print(f\"\\nARIMA model results for {company}:\")\n",
      "    forecast, mse = train_arima(y_train, y_test, steps=forecast_horizon)\n",
      "    \n",
      "    # Plot the results\n",
      "    plt.figure(figsize=(14, 7))\n",
      "    plt.plot(y_test.index[:forecast_horizon], y_test[:forecast_horizon], label='Actual')\n",
      "    plt.plot(y_test.index[:forecast_horizon], forecast, label='Forecast')\n",
      "    plt.title(f'ARIMA Forecast vs Actual for {company}')\n",
      "    plt.xlabel('Date')\n",
      "    plt.ylabel('Close Price')\n",
      "    plt.legend()\n",
      "    \n",
      "    # Save the plot\n",
      "    img_filename = f'{images_dir}/{img_number:03d}_{company}.png'\n",
      "    plt.savefig(img_filename)\n",
      "    print(f\"Saved plot for {company} as {img_filename}\")\n",
      "    \n",
      "    # Display the plot\n",
      "    plt.show()\n",
      "    \n",
      "    img_number += 1\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 12:\n",
      "\n",
      "\n",
      "\n",
      "# Function to calculate evaluation metrics\n",
      "def calculate_metrics(y_true, y_pred):\n",
      "    mse = mean_squared_error(y_true, y_pred)\n",
      "    rmse = np.sqrt(mse)\n",
      "    mae = mean_absolute_error(y_true, y_pred)\n",
      "    return mse, rmse, mae\n",
      "\n",
      "# Train and evaluate ARIMA model for each company\n",
      "metrics = {}\n",
      "forecast_horizon = 7  # You can extend this as needed\n",
      "img_number = 1  # Initialize image numbering\n",
      "\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    print(f\"\\nARIMA model results for {company}:\")\n",
      "    forecast, mse = train_arima(y_train, y_test, steps=forecast_horizon)\n",
      "    \n",
      "    # Calculate additional metrics\n",
      "    mse, rmse, mae = calculate_metrics(y_test[:forecast_horizon], forecast)\n",
      "    metrics[company] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
      "    print(f\"Evaluation Metrics for {company}: MSE={mse}, RMSE={rmse}, MAE={mae}\")\n",
      "    \n",
      "    # Plot the results\n",
      "    plt.figure(figsize=(14, 7))\n",
      "    plt.plot(y_test.index[:forecast_horizon], y_test[:forecast_horizon], label='Actual')\n",
      "    plt.plot(y_test.index[:forecast_horizon], forecast, label='Forecast')\n",
      "    plt.title(f'ARIMA Forecast vs Actual for {company}')\n",
      "    plt.xlabel('Date')\n",
      "    plt.ylabel('Close Price')\n",
      "    plt.legend()\n",
      "    \n",
      "    # Save the plot\n",
      "    img_filename = f'{images_dir}/{img_number:03d}_{company}_forecast.png'\n",
      "    plt.savefig(img_filename)\n",
      "    print(f\"Saved plot for {company} as {img_filename}\")\n",
      "    \n",
      "    # Display the plot\n",
      "    plt.show()\n",
      "    \n",
      "    img_number += 1\n",
      "\n",
      "# Display the evaluation metrics for all companies\n",
      "metrics_df = pd.DataFrame(metrics).T\n",
      "print(\"Evaluation Metrics for All Companies:\")\n",
      "display(metrics_df)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 13:\n",
      "\n",
      "# Function to train and evaluate ARIMA model for the entire test period\n",
      "def train_arima_full(y_train, y_test):\n",
      "    # Ensure the index is of DatetimeIndex type and set frequency\n",
      "    y_train.index = pd.DatetimeIndex(y_train.index)\n",
      "    y_train.index.freq = y_train.index.inferred_freq\n",
      "    y_test.index = pd.DatetimeIndex(y_test.index)\n",
      "    y_test.index.freq = y_test.index.inferred_freq\n",
      "    \n",
      "    model = ARIMA(y_train, order=(5, 1, 0))\n",
      "    model_fit = model.fit()\n",
      "    \n",
      "    # Forecast for the entire test period\n",
      "    forecast = model_fit.forecast(steps=len(y_test))\n",
      "    \n",
      "    return forecast\n",
      "\n",
      "# Train and evaluate ARIMA model for each company for the entire test period\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    print(f\"\\nARIMA model full results for {company}:\")\n",
      "    forecast = train_arima_full(y_train, y_test)\n",
      "    \n",
      "    # Plot the results\n",
      "    plt.figure(figsize=(14, 7))\n",
      "    plt.plot(y_test.index, y_test, label='Actual')\n",
      "    plt.plot(y_test.index, forecast, label='Forecast')\n",
      "    plt.title(f'ARIMA Forecast vs Actual for {company} (Full Test Period)')\n",
      "    plt.xlabel('Date')\n",
      "    plt.ylabel('Close Price')\n",
      "    plt.legend()\n",
      "    \n",
      "    # Save the plot\n",
      "    img_filename = f'{images_dir}/{img_number:03d}_{company}_full_forecast.png'\n",
      "    plt.savefig(img_filename)\n",
      "    print(f\"Saved full test period plot for {company} as {img_filename}\")\n",
      "    \n",
      "    # Display the plot\n",
      "    plt.show()\n",
      "    \n",
      "    img_number += 1\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 14:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 15:\n",
      "\n",
      "\n",
      "# # Suppress warnings\n",
      "# warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "# # Load the tweet dataset\n",
      "# tweets_df = pd.read_csv('/mnt/data/stocktweet.csv')\n",
      "\n",
      "# # Convert date column to datetime\n",
      "# tweets_df['date'] = pd.to_datetime(tweets_df['date'], format='%d/%m/%Y')\n",
      "\n",
      "# # Function to calculate sentiment polarity\n",
      "# def get_sentiment(tweet):\n",
      "#     analysis = TextBlob(tweet)\n",
      "#     return analysis.sentiment.polarity\n",
      "\n",
      "# # Apply the sentiment analysis function to the tweet text\n",
      "# tweets_df['sentiment'] = tweets_df['tweet'].apply(get_sentiment)\n",
      "\n",
      "# # Aggregate sentiment scores by date\n",
      "# daily_sentiment = tweets_df.groupby('date')['sentiment'].mean().reset_index()\n",
      "\n",
      "# # Convert `date` column in `daily_sentiment` to datetime\n",
      "# daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
      "\n",
      "# # List of companies and stock data dictionary\n",
      "# companies = ['AAPL', 'AMZN', 'GOOG', 'MSFT', 'TSLA']\n",
      "# stock_data = {}\n",
      "\n",
      "# for company in companies:\n",
      "#     company_csv_path = f'/mnt/data/{company}.csv'\n",
      "#     df_name = f'{company.lower()}_df'\n",
      "#     stock_data[company] = pd.read_csv(company_csv_path)\n",
      "    \n",
      "#     # Convert `Date` column to datetime\n",
      "#     stock_data[company]['Date'] = pd.to_datetime(stock_data[company]['Date'])\n",
      "    \n",
      "#     # Filter out rows with dates from 2019 in the stock data\n",
      "#     stock_data[company] = stock_data[company][stock_data[company]['Date'].dt.year != 2019]\n",
      "    \n",
      "#     # Create a variable with a name like goog_df\n",
      "#     globals()[df_name] = stock_data[company]\n",
      "    \n",
      "#     print(f\"First few rows of the {company} dataset:\")\n",
      "#     display(stock_data[company].head())\n",
      "\n",
      "# # Display the first few rows of the tweets dataset with sentiment scores\n",
      "# print(\"First few rows of the tweets dataset with sentiment scores:\")\n",
      "# display(tweets_df.head())\n",
      "\n",
      "# Function to create lag features\n",
      "def create_lag_features(df, lags, target_col):\n",
      "    for lag in lags:\n",
      "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
      "    return df\n",
      "\n",
      "# Define the lags and the target column\n",
      "lags = [1, 3, 7]\n",
      "target_col = 'Close'\n",
      "\n",
      "# Prepare the data for each company\n",
      "prepared_stock_data = {}\n",
      "for company in companies:\n",
      "    df = stock_data[company]\n",
      "    \n",
      "    # Create lag features for the Close price and sentiment\n",
      "    df = create_lag_features(df, lags, target_col)\n",
      "    df = create_lag_features(df, lags, 'sentiment')\n",
      "    \n",
      "    # Drop rows with NaN values created by the lag features\n",
      "    df = df.dropna()\n",
      "    \n",
      "    # Set Date as index\n",
      "    df = df.set_index('Date')\n",
      "    \n",
      "    # Split the data into training and testing sets\n",
      "    X = df.drop(columns=['Close'])\n",
      "    y = df['Close']\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
      "    \n",
      "    # Store the prepared data\n",
      "    prepared_stock_data[company] = (X_train, X_test, y_train, y_test)\n",
      "\n",
      "# Function to calculate evaluation metrics\n",
      "def calculate_metrics(y_test, forecast):\n",
      "    mse = mean_squared_error(y_test, forecast)\n",
      "    rmse = np.sqrt(mse)\n",
      "    mae = mean_absolute_error(y_test, forecast)\n",
      "    return mse, rmse, mae\n",
      "\n",
      "# Function to train ARIMA model and forecast\n",
      "def train_arima(y_train, y_test, steps):\n",
      "    model = ARIMA(y_train, order=(5,1,0))  # Adjust order as needed\n",
      "    model_fit = model.fit()\n",
      "    forecast = model_fit.forecast(steps=steps)\n",
      "    return forecast, model_fit.mse\n",
      "\n",
      "# Directory to save images\n",
      "images_dir = 'images'\n",
      "os.makedirs(images_dir, exist_ok=True)\n",
      "\n",
      "# Initialize a dictionary to store the results\n",
      "results = {}\n",
      "forecast_horizons = [1, 3, 7]  # Define your forecast horizons\n",
      "img_number = 1  # Initialize image numbering\n",
      "\n",
      "# Train and evaluate ARIMA model for each company and each forecast horizon\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    results[company] = {}\n",
      "    \n",
      "    for horizon in forecast_horizons:\n",
      "        print(f\"\\nARIMA model results for {company} - {horizon}-day forecast:\")\n",
      "        forecast, _ = train_arima(y_train, y_test, steps=horizon)\n",
      "        \n",
      "        # Calculate additional metrics\n",
      "        mse, rmse, mae = calculate_metrics(y_test[:horizon], forecast)\n",
      "        results[company][horizon] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
      "        \n",
      "        # Debug statements\n",
      "        print(f\"Forecast for {company} - {horizon}-day forecast:\\n{forecast}\")\n",
      "        print(f\"Actual values for {company} - {horizon}-day forecast:\\n{y_test[:horizon]}\")\n",
      "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
      "        \n",
      "        # Ensure there are enough data points for plotting\n",
      "        if len(y_test[:horizon]) == 0:\n",
      "            print(f\"Not enough data points for {company} - {horizon}-day forecast\")\n",
      "            continue\n",
      "        \n",
      "        # Plot the results\n",
      "        plt.figure(figsize=(14, 7))\n",
      "        plt.plot(y_test.index[:horizon], y_test[:horizon], label='Actual', marker='o')\n",
      "        plt.plot(y_test.index[:horizon], forecast, label='Forecast', marker='x')\n",
      "        plt.title(f'ARIMA {horizon}-day Forecast vs Actual for {company}')\n",
      "        plt.xlabel('Date')\n",
      "        plt.ylabel('Close Price')\n",
      "        plt.legend()\n",
      "        \n",
      "        # Save the plot\n",
      "        img_filename = f'{images_dir}/{img_number:03d}_{company}_{horizon}day.png'\n",
      "        plt.savefig(img_filename)\n",
      "        print(f\"Saved plot for {company} {horizon}-day forecast as {img_filename}\")\n",
      "        \n",
      "        plt.show()\n",
      "        plt.close()\n",
      "        \n",
      "        img_number += 1\n",
      "\n",
      "# Display the results in a tabular format\n",
      "summary = []\n",
      "for company in results:\n",
      "    for horizon in results[company]:\n",
      "        summary.append([company, horizon, results[company][horizon]['MSE'], results[company][horizon]['RMSE'], results[company][horizon]['MAE']])\n",
      "\n",
      "summary_df = pd.DataFrame(summary, columns=['Company', 'Horizon', 'MSE', 'RMSE', 'MAE'])\n",
      "summary_df = summary_df.pivot(index='Company', columns='Horizon')\n",
      "summary_df\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 16:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 17:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 18:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 19:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 20:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 21:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 22:\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Function to calculate additional evaluation metrics\n",
      "def calculate_metrics(y_test, forecast):\n",
      "    mse = mean_squared_error(y_test, forecast)\n",
      "    rmse = np.sqrt(mse)\n",
      "    mae = np.mean(np.abs(y_test - forecast))\n",
      "    return mse, rmse, mae\n",
      "\n",
      "# Initialize a dictionary to store the results\n",
      "results = {company: {horizon: {} for horizon in forecast_horizons} for company in companies}\n",
      "\n",
      "# Train and evaluate ARIMA model for each company and each forecast horizon\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    for horizon in forecast_horizons:\n",
      "        print(f\"\\nARIMA model results for {company} - {horizon}-day forecast:\")\n",
      "        forecast, mse = train_arima(y_train, y_test, steps=horizon)\n",
      "        \n",
      "        # Calculate additional metrics\n",
      "        mse, rmse, mae = calculate_metrics(y_test[:horizon], forecast)\n",
      "        results[company][horizon] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
      "        \n",
      "        # Debug statements\n",
      "        print(f\"Forecast for {company} - {horizon}-day forecast:\\n{forecast}\")\n",
      "        print(f\"Actual values for {company} - {horizon}-day forecast:\\n{y_test[:horizon]}\")\n",
      "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
      "        \n",
      "        # Ensure there are enough data points for plotting\n",
      "        if len(y_test[:horizon]) == 0:\n",
      "            print(f\"Not enough data points for {company} - {horizon}-day forecast\")\n",
      "            continue\n",
      "        \n",
      "        # Plot the results\n",
      "        plt.figure(figsize=(14, 7))\n",
      "        plt.plot(y_test.index[:horizon], y_test[:horizon], label='Actual', marker='o')\n",
      "        plt.plot(y_test.index[:horizon], forecast, label='Forecast', marker='x')\n",
      "        plt.title(f'ARIMA {horizon}-day Forecast vs Actual for {company}')\n",
      "        plt.xlabel('Date')\n",
      "        plt.ylabel('Close Price')\n",
      "        plt.legend()\n",
      "        \n",
      "        # Save the plot\n",
      "        img_filename = f'{images_dir}/{img_number:03d}_{company}_{horizon}day.png'\n",
      "        plt.savefig(img_filename)\n",
      "        print(f\"Saved plot for {company} {horizon}-day forecast as {img_filename}\")\n",
      "        \n",
      "        plt.show()\n",
      "        plt.close()\n",
      "        \n",
      "        img_number += 1\n",
      "\n",
      "# Display the results in a tabular format\n",
      "import pandas as pd\n",
      "\n",
      "# Prepare a DataFrame to summarize the results\n",
      "summary = []\n",
      "for company in results:\n",
      "    for horizon in results[company]:\n",
      "        summary.append([company, horizon, results[company][horizon]['MSE'], results[company][horizon]['RMSE'], results[company][horizon]['MAE']])\n",
      "\n",
      "summary_df = pd.DataFrame(summary, columns=['Company', 'Horizon', 'MSE', 'RMSE', 'MAE'])\n",
      "summary_df = summary_df.pivot(index='Company', columns='Horizon')\n",
      "summary_df\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 23:\n",
      "\n",
      "from statsmodels.tsa.arima.model import ARIMA\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "\n",
      "# Create the images directory if it doesn't exist\n",
      "images_dir = 'images'\n",
      "if not os.path.exists(images_dir):\n",
      "    os.makedirs(images_dir)\n",
      "\n",
      "# Function to train and evaluate ARIMA model\n",
      "def train_arima(y_train, y_test, steps=7):\n",
      "    # Ensure the index is of DatetimeIndex type and set frequency\n",
      "    y_train.index = pd.DatetimeIndex(y_train.index)\n",
      "    y_test.index = pd.DatetimeIndex(y_test.index)\n",
      "    \n",
      "    model = ARIMA(y_train, order=(5, 1, 0))\n",
      "    model_fit = model.fit()\n",
      "    \n",
      "    # Forecast\n",
      "    forecast = model_fit.forecast(steps=steps)\n",
      "    \n",
      "    # Evaluate the model\n",
      "    mse = mean_squared_error(y_test[:steps], forecast)\n",
      "    print(f'Mean Squared Error: {mse}')\n",
      "    \n",
      "    return forecast, mse\n",
      "\n",
      "# Forecast horizons\n",
      "forecast_horizons = [1, 3, 7]  # 1 day, 3 days, 7 days\n",
      "img_number = 1  # Initialize image numbering\n",
      "\n",
      "# Train and evaluate ARIMA model for each company and each forecast horizon\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    for horizon in forecast_horizons:\n",
      "        print(f\"\\nARIMA model results for {company} - {horizon}-day forecast:\")\n",
      "        forecast, mse = train_arima(y_train, y_test, steps=horizon)\n",
      "        \n",
      "        # Debug statements\n",
      "        print(f\"Forecast for {company} - {horizon}-day forecast:\\n{forecast}\")\n",
      "        print(f\"Actual values for {company} - {horizon}-day forecast:\\n{y_test[:horizon]}\")\n",
      "        print(f\"Length of forecast: {len(forecast)}\")\n",
      "        print(f\"Length of actual values: {len(y_test[:horizon])}\")\n",
      "        \n",
      "        # Ensure there are enough data points for plotting\n",
      "        if len(y_test[:horizon]) == 0:\n",
      "            print(f\"Not enough data points for {company} - {horizon}-day forecast\")\n",
      "            continue\n",
      "        \n",
      "        # Plot the results\n",
      "        plt.figure(figsize=(14, 7))\n",
      "        plt.plot(y_test.index[:horizon], y_test[:horizon], label='Actual', marker='o')\n",
      "        plt.plot(y_test.index[:horizon], forecast, label='Forecast', marker='x')\n",
      "        plt.title(f'ARIMA {horizon}-day Forecast vs Actual for {company}')\n",
      "        plt.xlabel('Date')\n",
      "        plt.ylabel('Close Price')\n",
      "        plt.legend()\n",
      "        \n",
      "        # Save the plot\n",
      "        img_filename = f'{images_dir}/{img_number:03d}_{company}_{horizon}day.png'\n",
      "        plt.savefig(img_filename)\n",
      "        print(f\"Saved plot for {company} {horizon}-day forecast as {img_filename}\")\n",
      "        \n",
      "        plt.show()\n",
      "        plt.close()\n",
      "        \n",
      "        img_number += 1\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 24:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Function to calculate additional evaluation metrics\n",
      "def calculate_metrics(y_test, forecast):\n",
      "    mse = mean_squared_error(y_test, forecast)\n",
      "    rmse = np.sqrt(mse)\n",
      "    mae = np.mean(np.abs(y_test - forecast))\n",
      "    return mse, rmse, mae\n",
      "\n",
      "# Initialize a dictionary to store the results\n",
      "results = {company: {horizon: {} for horizon in forecast_horizons} for company in companies}\n",
      "\n",
      "# Train and evaluate ARIMA model for each company and each forecast horizon\n",
      "for company in companies:\n",
      "    X_train, X_test, y_train, y_test = prepared_stock_data[company]\n",
      "    \n",
      "    for horizon in forecast_horizons:\n",
      "        print(f\"\\nARIMA model results for {company} - {horizon}-day forecast:\")\n",
      "        forecast, mse = train_arima(y_train, y_test, steps=horizon)\n",
      "        \n",
      "        # Calculate additional metrics\n",
      "        mse, rmse, mae = calculate_metrics(y_test[:horizon], forecast)\n",
      "        results[company][horizon] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
      "        \n",
      "        # Debug statements\n",
      "        print(f\"Forecast for {company} - {horizon}-day forecast:\\n{forecast}\")\n",
      "        print(f\"Actual values for {company} - {horizon}-day forecast:\\n{y_test[:horizon]}\")\n",
      "        print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
      "\n",
      "# Display the results in a tabular format\n",
      "\n",
      "# Prepare a DataFrame to summarize the results\n",
      "summary = []\n",
      "for company in results:\n",
      "    for horizon in results[company]:\n",
      "        summary.append([company, horizon, results[company][horizon]['MSE'], results[company][horizon]['RMSE'], results[company][horizon]['MAE']])\n",
      "\n",
      "summary_df = pd.DataFrame(summary, columns=['Company', 'Horizon', 'MSE', 'RMSE', 'MAE'])\n",
      "summary_df = summary_df.pivot(index='Company', columns='Horizon')\n",
      "summary_df\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 25:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 26:\n",
      "\n",
      "notebook_end_time = time.time()\n",
      "elapsed_time = notebook_end_time - notebook_start_time\n",
      "print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 27:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "All code cells have been copied to the clipboard.\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import pyperclip\n",
    "\n",
    "# Path to your Jupyter Notebook file\n",
    "notebook_path = 'PricesTweets.ipynb'\n",
    "\n",
    "# Read the notebook file\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Extract all code cells\n",
    "code_cells = [cell['source'] for cell in notebook.cells if cell.cell_type == 'code']\n",
    "\n",
    "# Join all code cells into a single string\n",
    "code_cells_text = '\\n\\n'.join(code_cells)\n",
    "\n",
    "# Copy to clipboard\n",
    "pyperclip.copy(code_cells_text)\n",
    "\n",
    "# Display the extracted code cells (optional)\n",
    "for i, code in enumerate(code_cells):\n",
    "    print(f'Code Cell {i+1}:\\n')\n",
    "    print(code)\n",
    "    print('\\n' + '='*80 + '\\n')\n",
    "\n",
    "print(\"All code cells have been copied to the clipboard.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "687431b6-8ae9-412e-9d6e-c527a4f83c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Cell 1:\n",
      "\n",
      "# pip install holidays\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 2:\n",
      "\n",
      "#import necessary libraries and files \n",
      "\n",
      "\n",
      "import time\n",
      "# Start time for notebook execution\n",
      "notebook_start_time = time.time()\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import warnings\n",
      "\n",
      "import datetime as dt\n",
      "warnings.filterwarnings('ignore')\n",
      "import matplotlib.pyplot as plt\n",
      "import folium\n",
      "import sklearn\n",
      "import seaborn as sns\n",
      "import os\n",
      "from os import listdir\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from matplotlib import pyplot as plt\n",
      "from sklearn.cluster import KMeans\n",
      "import re\n",
      "\n",
      "## Feature Engineering Dublin Bikes Usage Data\n",
      "import numpy as np\n",
      "import datetime as dt\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import holidays\n",
      "\n",
      "import os\n",
      "import glob\n",
      "\n",
      "\n",
      "import holidays\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 3:\n",
      "\n",
      "# In this project, organizing the directory structure programmatically\n",
      "# is crucial for maintaining a clean and efficient workspace. By defining\n",
      "# directories as key-value pairs in a dictionary, we ensure that directory \n",
      "# management is streamlined and consistent throughout the project.\n",
      "\n",
      "# The 'directories' dictionary stores descriptive variable names (keys)\n",
      "# and their corresponding directory paths (values), promoting clarity \n",
      "# and ease of access in the code.\n",
      "\n",
      "directories = {\n",
      "    'data': 'data',\n",
      "      'chunks': 'chunks',\n",
      "    # 'chunks': 'chunks_200',    \n",
      "    'weather': 'weather',\n",
      "    'capitalbikes_csvs': 'capitalbikes_csvs',\n",
      "    'images': 'images',  \n",
      "}\n",
      "\n",
      "\n",
      "# This loop creates the necessary directories if they don't exist,\n",
      "# and associates each directory with a global variable for easy reference \n",
      "# throughout the notebook. This is part of maintaining high code quality \n",
      "# standards, ensuring the project remains organized and manageable.\n",
      "\n",
      "for key, directory in directories.items():\n",
      "    os.makedirs(directory, exist_ok=True)\n",
      "    globals()[key] = directory\n",
      "\n",
      "# Provide feedublinbikes_dfack \n",
      "print(\"Data directories exist or have been created.\")\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 4:\n",
      "\n",
      "## initialise indices\n",
      "image_index = 200\n",
      "data_index= 200\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 5:\n",
      "\n",
      "\n",
      "\n",
      "def save_files(DataFrame, descriptor):\n",
      "    \"\"\"\n",
      "    Saves the full DataFrame in both CSV and Parquet formats and a sample (up to 10,000 rows) \n",
      "    in both CSV and Parquet formats. Reports the sizes of the saved files.\n",
      "\n",
      "    Parameters:\n",
      "    DataFrame (pd.DataFrame): The DataFrame to save.\n",
      "    descriptor (str): Label for the file.\n",
      "\n",
      "    - Files are saved in the 'data/' directory and only if they don't already exist.\n",
      "    \"\"\"\n",
      "\n",
      "    # Construct base filenames based on the descriptor\n",
      "    base_filename = f\"{descriptor}\"\n",
      "\n",
      "    # Define the file paths for saving the DataFrame in full and sample formats \n",
      "    csv_filename = f\"data/{base_filename}.csv\"\n",
      "    parquet_filename = f\"data/{base_filename}.parquet\"\n",
      "    sample_csv_filename = f\"data/{base_filename}_sample.csv\"\n",
      "    sample_parquet_filename = f\"data/{base_filename}_sample.parquet\"\n",
      "\n",
      "    # Check if all target files already exist to avoid redundant saving\n",
      "    if not (os.path.exists(csv_filename) and os.path.exists(parquet_filename) and\n",
      "            os.path.exists(sample_csv_filename) and os.path.exists(sample_parquet_filename)):\n",
      "        \n",
      "        # Save the full DataFrame to CSV format\n",
      "        DataFrame.to_csv(csv_filename, index=False)\n",
      "        csv_size_mb = os.path.getsize(csv_filename) / (1024 * 1024)\n",
      "        print(f\"DataFrame saved as CSV: {csv_filename} ({csv_size_mb:.2f} MB)\")\n",
      "\n",
      "        # Save the full DataFrame to Parquet format\n",
      "        DataFrame.to_parquet(parquet_filename, index=False)\n",
      "        parquet_size_mb = os.path.getsize(parquet_filename) / (1024 * 1024)\n",
      "        print(f\"DataFrame saved as Parquet: {parquet_filename} ({parquet_size_mb:.2f} MB)\")\n",
      "\n",
      "        # Create and save a sample of the DataFrame (up to 10,000 rows)\n",
      "        sample_size = min(10000, len(DataFrame))\n",
      "        sample_df = DataFrame.sample(n=sample_size)\n",
      "        \n",
      "        sample_df.to_csv(sample_csv_filename, index=False)\n",
      "        sample_csv_size_mb = os.path.getsize(sample_csv_filename) / (1024 * 1024)\n",
      "        print(f\"Sample DataFrame saved as CSV: {sample_csv_filename} ({sample_csv_size_mb:.2f} MB)\")\n",
      "        \n",
      "        sample_df.to_parquet(sample_parquet_filename, index=False)\n",
      "        sample_parquet_size_mb = os.path.getsize(sample_parquet_filename) / (1024 * 1024)\n",
      "        print(f\"Sample DataFrame saved as Parquet: {sample_parquet_filename} ({sample_parquet_size_mb:.2f} MB)\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "def save_parquet(df, filename, directory='data'):\n",
      "    \"\"\"\n",
      "    Save a DataFrame to a Parquet file at the specified directory and print the file size.\n",
      "\n",
      "    Parameters:\n",
      "    - df (pandas.DataFrame): The DataFrame to save.\n",
      "    - filename (str): The name of the file (without extension).\n",
      "    - directory (str): The directory where the file will be saved (default is 'data').\n",
      "    \"\"\"\n",
      "    # Ensure the directory exists\n",
      "    if not os.path.exists(directory):\n",
      "        os.makedirs(directory)\n",
      "\n",
      "    # Construct the full filepath with snake_case filename\n",
      "    snake_case_filename = filename.lower().replace(' ', '_') + '.parquet'\n",
      "    filepath = os.path.join(directory, snake_case_filename)\n",
      "\n",
      "    # Save the DataFrame as a Parquet file\n",
      "    df.to_parquet(filepath, index=False)\n",
      "\n",
      "    # Get the size of the file in bytes\n",
      "    file_size = os.path.getsize(filepath)\n",
      "\n",
      "    # Convert the size to a more readable format (e.g., KB, MB)\n",
      "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
      "        if file_size < 1024.0:\n",
      "            break\n",
      "        file_size /= 1024.0\n",
      "\n",
      "    # Print the location and size of the file\n",
      "    print(f\"DataFrame saved to '{filepath}'\")\n",
      "    print(f\"File size: {file_size:.2f} {unit}\")\n",
      "\n",
      "\n",
      "\n",
      "def save_chunks(df, descriptor):\n",
      "    \"\"\"\n",
      "    Saves a DataFrame in chunks of 1,000,000 rows each as Parquet files in a directory named after the descriptor.\n",
      "\n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to save in chunks.\n",
      "    descriptor (str): The descriptor used for naming the output directory and chunk files.\n",
      "    \"\"\"\n",
      "    # Set the chunk size to 1,000,000\n",
      "    chunk_size = 1000000\n",
      "\n",
      "    # Create the directory path based on the descriptor inside the 'chunks' directory\n",
      "    output_dir = os.path.join('chunks', descriptor)\n",
      "    \n",
      "    # Ensure the output directory exists\n",
      "    if not os.path.exists(output_dir):\n",
      "        os.makedirs(output_dir)\n",
      "    \n",
      "    # Determine the number of chunks\n",
      "    num_chunks = (len(df) // chunk_size) + 1\n",
      "    \n",
      "    for i in range(num_chunks):\n",
      "        # Calculate start and end indices for each chunk\n",
      "        start = i * chunk_size\n",
      "        end = min((i + 1) * chunk_size, len(df))\n",
      "        \n",
      "        # Slice the DataFrame and write it to a new Parquet file\n",
      "        chunk_df = df.iloc[start:end]\n",
      "        chunk_file_path = f\"{output_dir}/{descriptor}_{i}.parquet\"\n",
      "        chunk_df.to_parquet(chunk_file_path, index=False)\n",
      "        print(f\"Chunk {i} saved to {chunk_file_path}\")\n",
      "\n",
      "\n",
      "def read_chunks(descriptor):\n",
      "    \"\"\"\n",
      "    Reads and concatenates all Parquet files in the directory named after the descriptor within the 'chunks' directory into a single DataFrame.\n",
      "\n",
      "    Parameters:\n",
      "    descriptor (str): The descriptor used to identify the folder containing the Parquet files.\n",
      "\n",
      "    Returns:\n",
      "    pd.DataFrame: A DataFrame containing the concatenated data from all Parquet files in the directory.\n",
      "    \"\"\"\n",
      "    # Define the directory path based on the descriptor\n",
      "    input_dir = os.path.join('chunks', descriptor)\n",
      "    \n",
      "    # Get a list of all Parquet files in the directory\n",
      "    files = glob.glob(f\"{input_dir}/*.parquet\")\n",
      "    \n",
      "    print(\"Files found:\", files)  # Debugging line\n",
      "    \n",
      "    if not files:\n",
      "        raise ValueError(\"No Parquet files found in the specified directory.\")\n",
      "    \n",
      "    # Read and concatenate all Parquet files\n",
      "    df_list = [pd.read_parquet(file) for file in files]\n",
      "    \n",
      "    if not df_list:\n",
      "        raise ValueError(\"No DataFrames were read from the Parquet files.\")\n",
      "    \n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    \n",
      "    return combined_df\n",
      "\n",
      "\n",
      "def save_sample_csv(DataFrame, descriptor):\n",
      "    \"\"\"\n",
      "    Saves a sample (up to 10,000 rows) of the DataFrame in CSV format.\n",
      "\n",
      "    Parameters:\n",
      "    DataFrame (pd.DataFrame): The DataFrame to sample and save.\n",
      "    descriptor (str): Label for the file, used to construct the filename.\n",
      "\n",
      "    - File is saved in the 'data/' directory as 'descriptor_sample.csv'.\n",
      "    \"\"\"\n",
      "    \n",
      "    import os\n",
      "\n",
      "    # Construct the filename using the descriptor\n",
      "    sample_csv_filename = f\"data/{descriptor}_sample.csv\"\n",
      "\n",
      "    # Check if the target file already exists to avoid redundant saving\n",
      "    if not os.path.exists(sample_csv_filename):\n",
      "\n",
      "        # Create and save a sample of the DataFrame (up to 10,000 rows)\n",
      "        sample_size = min(10000, len(DataFrame))\n",
      "        sample_df = DataFrame.sample(n=sample_size)\n",
      "        \n",
      "        sample_df.to_csv(sample_csv_filename, index=False)\n",
      "        sample_csv_size_mb = os.path.getsize(sample_csv_filename) / (1024 * 1024)\n",
      "        print(f\"Sample DataFrame saved as CSV: {sample_csv_filename} ({sample_csv_size_mb:.2f} MB)\")\n",
      "\n",
      "def read_par(data, descriptor):\n",
      "    # Define the file path\n",
      "    file_path = os.path.join(data, f\"{descriptor}.parquet\")\n",
      "    \n",
      "    # Check if the file exists\n",
      "    if not os.path.exists(file_path):\n",
      "        raise FileNotFoundError(f\"No file found at {file_path}\")\n",
      "    \n",
      "    # Load the DataFrame from the Parquet file\n",
      "    df = pd.read_parquet(file_path)\n",
      "    \n",
      "    print(f\"DataFrame loaded from {file_path}\")\n",
      "    return df\n",
      "\n",
      "\n",
      "\n",
      "def save_csv(DataFrame, filename):\n",
      "    \"\"\"\n",
      "    Saves the DataFrame in CSV format and reports the file size and name.\n",
      "\n",
      "    Parameters:\n",
      "    - DataFrame (pd.DataFrame): The DataFrame to save.\n",
      "    - filename (str): The name of the file to save the DataFrame as, without the '.csv' extension.\n",
      "\n",
      "    The file is saved in the 'data/' directory with a '.csv' extension.\n",
      "    \"\"\"\n",
      "    # Ensure the 'data' directory exists\n",
      "    os.makedirs('data', exist_ok=True)\n",
      "\n",
      "    # Construct the full file path\n",
      "    full_filename = f\"data/{filename}.csv\"\n",
      "\n",
      "    # Save the DataFrame to CSV\n",
      "    DataFrame.to_csv(full_filename, index=False)\n",
      "\n",
      "    # Get the file size in MB\n",
      "    file_size_mb = os.path.getsize(full_filename) / (1024 * 1024)\n",
      "\n",
      "    # Report the file name and its size\n",
      "    print(f\"DataFrame saved as CSV: {full_filename} ({file_size_mb:.2f} MB)\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 6:\n",
      "\n",
      "\n",
      "def save_image(descriptor):\n",
      "    \"\"\"\n",
      "    Save the current figure with a filename that includes only the descriptor (without an index).\n",
      "    \n",
      "    Parameters:\n",
      "    - descriptor: A string to include in the filename for contextual information.\n",
      "    \"\"\"\n",
      "    # Construct the filename using only the descriptor and save to the directory specified by `images`\n",
      "    filename = os.path.join(images, f'{descriptor}.png')\n",
      "    \n",
      "    # Save the figure\n",
      "    plt.savefig(filename)\n",
      "    \n",
      "    print(f\"Image saved as {filename}\")\n",
      "\n",
      "\n",
      "def save_index_image(descriptor):\n",
      "    \"\"\"\n",
      "    Save the current figure with a filename that includes the image index and descriptor.\n",
      "    \n",
      "    Parameters:\n",
      "    - descriptor: A string to include in the filename for contextual information.\n",
      "    \"\"\"\n",
      "    global image_index  # Access the global image_index variable\n",
      "    \n",
      "    # Construct the filename using image_index and descriptor, and save to the directory specified by `images`\n",
      "    filename = os.path.join(images, f'{image_index}_{descriptor}.png')\n",
      "    \n",
      "    # Save the figure\n",
      "    plt.savefig(filename)\n",
      "    \n",
      "    # Increment the image index after use\n",
      "    image_index += 5\n",
      "\n",
      "    print(f\"Indexed image saved as {filename}\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def save_box(descriptor, bbox_inches='tight'):\n",
      "    \"\"\"\n",
      "    Save the current figure with a filename that includes the image index and descriptor.\n",
      "    \n",
      "    Parameters:\n",
      "    - descriptor: A string to include in the filename for contextual information.\n",
      "    - bbox_inches: Controls the size of the bounding box for the saved figure.\n",
      "    \"\"\"\n",
      "    global image_index  # Access the global image_index variable\n",
      "    \n",
      "    # Construct the filename using image_index and descriptor\n",
      "    filename = os.path.join(images, f'{image_index}_{descriptor}.png')\n",
      "    \n",
      "    # Save the figure with bbox_inches parameter to ensure the legend is not cut off\n",
      "    plt.savefig(filename, bbox_inches=bbox_inches)\n",
      "    \n",
      "    # Increment the image index after use\n",
      "    image_index += 1\n",
      "\n",
      "\n",
      "def read_chunks(descriptor):\n",
      "    \"\"\"\n",
      "    Reads and concatenates all Parquet files in the directory named after the descriptor within the 'chunks' directory into a single DataFrame.\n",
      "\n",
      "    Parameters:\n",
      "    descriptor (str): The descriptor used to identify the folder containing the Parquet files.\n",
      "\n",
      "    Returns:\n",
      "    pd.DataFrame: A DataFrame containing the concatenated data from all Parquet files in the directory.\n",
      "    \"\"\"\n",
      "    # Define the directory path based on the descriptor\n",
      "    input_dir = os.path.join('chunks', descriptor)\n",
      "    \n",
      "    # Get a list of all Parquet files in the directory\n",
      "    files = glob.glob(f\"{input_dir}/*.parquet\")\n",
      "    \n",
      "    print(\"Files found:\", files)  # Debugging line\n",
      "    \n",
      "    if not files:\n",
      "        raise ValueError(\"No Parquet files found in the specified directory.\")\n",
      "    \n",
      "    # Read and concatenate all Parquet files\n",
      "    df_list = [pd.read_parquet(file) for file in files]\n",
      "    \n",
      "    if not df_list:\n",
      "        raise ValueError(\"No DataFrames were read from the Parquet files.\")\n",
      "    \n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    \n",
      "    return combined_df\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 7:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def escape_latex(text):\n",
      "    \"\"\"\n",
      "    Escapes LaTeX special characters in a string.\n",
      "    \"\"\"\n",
      "    special_chars = {\n",
      "        '&': r'\\&', '%': r'\\%', '$': r'\\$', '#': r'\\#', '_': r'\\_', \n",
      "        '{': r'\\{', '}': r'\\}', '~': r'\\textasciitilde{}', '^': r'\\textasciicircum{}', '\\\\': r'\\\\'\n",
      "    }\n",
      "    for char, escape in special_chars.items():\n",
      "        text = text.replace(char, escape)\n",
      "    return text\n",
      "\n",
      "def latex_table(df):\n",
      "    \"\"\"\n",
      "    Convert the entire DataFrame into a LaTeX table and copy the result to the clipboard.\n",
      "    \n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to convert to a LaTeX table.\n",
      "    \"\"\"\n",
      "    # Ensure column headers are strings for LaTeX processing\n",
      "    df.columns = [escape_latex(str(col)) for col in df.columns]\n",
      "    \n",
      "    # Escape LaTeX special characters in the data\n",
      "    df = df.applymap(lambda x: escape_latex(str(x)))\n",
      "\n",
      "    # Round numeric data to 3 decimal places if applicable\n",
      "    df = df.applymap(lambda x: f'{float(x):.3f}' if isinstance(x, (float, np.float64, np.float32)) else x)\n",
      "    \n",
      "    # Start building the LaTeX table\n",
      "    latex_code = \"\\\\begin{table}[h!]\\n\\\\centering\\n\\\\caption{Your Caption Here}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\"\n",
      "    latex_code += \"\\\\begin{tabular}{\" + \"l|\" * (len(df.columns) - 1) + \"l}\\n\\\\hline\\n\"\n",
      "\n",
      "    # Add the headers with rotation\n",
      "    headers = \" & \".join([f\"\\\\rotatebox{{90}}{{\\\\textbf{{{col}}}}}\" for col in df.columns]) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
      "    latex_code += headers\n",
      "\n",
      "    # Add the rows\n",
      "    for _, row in df.iterrows():\n",
      "        row_data = \" & \".join([str(val) for val in row]) + \" \\\\\\\\\\n\"\n",
      "        latex_code += row_data\n",
      "    latex_code += \"\\\\hline\\n\"\n",
      "\n",
      "    # End the table\n",
      "    latex_code += \"\\\\end{tabular}%\\n\\\\end{table}\"\n",
      "\n",
      "    # Copy the generated LaTeX code to the clipboard\n",
      "    pd.DataFrame([latex_code]).to_clipboard(index=False, header=False)\n",
      "    print(\"LaTeX table for the entire DataFrame copied to clipboard!\")\n",
      "\n",
      "# Example usage\n",
      "# Assuming 'df' is your DataFrame\n",
      "# latex_table(df)\n",
      "\n",
      "\n",
      "def latex_head(df):\n",
      "    \"\"\"\n",
      "    Convert the first 5 rows of a DataFrame into a LaTeX table and copy the result to the clipboard.\n",
      "    \n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to convert to a LaTeX table.\n",
      "    \"\"\"\n",
      "    # Limit the DataFrame to the first 5 rows\n",
      "    df = df.head(5)\n",
      "    \n",
      "    # Ensure column headers are strings for LaTeX processing\n",
      "    df.columns = [escape_latex(str(col)) for col in df.columns]\n",
      "    \n",
      "    # Escape LaTeX special characters in the data\n",
      "    df = df.applymap(lambda x: escape_latex(str(x)))\n",
      "\n",
      "    # Round numeric data to 3 decimal places if applicable\n",
      "    df = df.applymap(lambda x: f'{float(x):.3f}' if isinstance(x, (float, np.float64, np.float32)) else x)\n",
      "    \n",
      "    # Start building the LaTeX table\n",
      "    latex_code = \"\\\\begin{table}[h!]\\n\\\\centering\\n\\\\caption{Your Caption Here}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\"\n",
      "    latex_code += \"\\\\begin{tabular}{\" + \"l|\" * (len(df.columns) - 1) + \"l}\\n\\\\hline\\n\"\n",
      "\n",
      "    # Add the headers with rotation\n",
      "    headers = \" & \".join([f\"\\\\rotatebox{{90}}{{\\\\textbf{{{col}}}}}\" for col in df.columns]) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
      "    latex_code += headers\n",
      "\n",
      "    # Add the rows\n",
      "    for _, row in df.iterrows():\n",
      "        row_data = \" & \".join([str(val) for val in row]) + \" \\\\\\\\\\n\"\n",
      "        latex_code += row_data\n",
      "    latex_code += \"\\\\hline\\n\"\n",
      "\n",
      "    # End the table\n",
      "    latex_code += \"\\\\end{tabular}%\\n\\\\end{table}\"\n",
      "\n",
      "    # Copy the generated LaTeX code to the clipboard\n",
      "    pd.DataFrame([latex_code]).to_clipboard(index=False, header=False)\n",
      "    print(\"LaTeX table for the first 5 rows copied to clipboard!\")\n",
      "\n",
      "# Example usage\n",
      "# Assuming 'df' is your DataFrame\n",
      "# head_table(df)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "def head_table(df):\n",
      "    \"\"\"\n",
      "    Convert the first 5 rows of a DataFrame into a LaTeX table and copy the result to the clipboard.\n",
      "\n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to convert to a LaTeX table.\n",
      "    \"\"\"\n",
      "    # Limit the DataFrame to the first 5 rows\n",
      "    df = df.head(5)\n",
      "    \n",
      "    # Start building the LaTeX table\n",
      "    latex_code = \"\\\\begin{table}[h!]\\n\\\\centering\\n\\\\caption{Your Caption Here}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\"\n",
      "    latex_code += \"\\\\begin{tabular}{\" + \"|\".join([\"l\"] * len(df.columns)) + \"|}\\n\\\\hline\\n\"\n",
      "\n",
      "    # Add the headers with rotation\n",
      "    headers = \" & \".join([f\"\\\\rotatebox{{90}}{{\\\\textbf{{{col}}}}}\" for col in df.columns]) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
      "    latex_code += headers\n",
      "\n",
      "    # Add the rows\n",
      "    for _, row in df.iterrows():\n",
      "        row_data = \" & \".join([str(val) for val in row]) + \" \\\\\\\\\\n\"\n",
      "        latex_code += row_data\n",
      "    latex_code += \"\\\\hline\\n\"\n",
      "\n",
      "    # End the table\n",
      "    latex_code += \"\\\\end{tabular}%\\n\\\\end{table}\"\n",
      "\n",
      "    # Copy the generated LaTeX code to the clipboard\n",
      "    pd.DataFrame([latex_code]).to_clipboard(index=False, header=False)\n",
      "    print(\"LaTeX table for the first 5 rows copied to clipboard!\")\n",
      "\n",
      "# Example usage\n",
      "# Assuming 'df' is your DataFrame\n",
      "# head_table(df)\n",
      "\n",
      "\n",
      "def memory_mb(df):\n",
      "    \"\"\"\n",
      "    Display the DataFrame info and detailed memory usage.\n",
      "\n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to analyze.\n",
      "    \"\"\"\n",
      "    # Use the built-in .info() method to display basic info\n",
      "    df.info(memory_usage='deep')\n",
      "    \n",
      "    # Calculate and print total memory usage\n",
      "    total_memory_usage = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
      "    print(f\"\\nTotal memory usage: {total_memory_usage:.2f} MB\")\n",
      "    \n",
      "    # Calculate and display memory usage per column\n",
      "    column_memory_usage = (df.memory_usage(deep=True) / (1024 ** 2)).round(2)\n",
      "    print(\"\\nMemory usage per column (MB):\")\n",
      "    print(column_memory_usage)\n",
      "\n",
      "\n",
      "def memory_kb(df):\n",
      "    \"\"\"\n",
      "    Display the DataFrame info and detailed memory usage.\n",
      "\n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to analyze.\n",
      "    \"\"\"\n",
      "    # Use the built-in .info() method to display basic info\n",
      "    df.info(memory_usage='deep')\n",
      "    \n",
      "    # Calculate and print total memory usage\n",
      "    total_memory_usage = df.memory_usage(deep=True).sum() / (1024)\n",
      "    print(f\"\\nTotal memory usage: {total_memory_usage:.2f} MB\")\n",
      "    \n",
      "    # Calculate and display memory usage per column\n",
      "    column_memory_usage = (df.memory_usage(deep=True) / (1024)).round(2)\n",
      "    print(\"\\nMemory usage per column (kB):\")\n",
      "    print(column_memory_usage)\n",
      "\n",
      "\n",
      "def memory_b(df):\n",
      "    \"\"\"\n",
      "    Display the DataFrame info and detailed memory usage.\n",
      "\n",
      "    Parameters:\n",
      "    df (pd.DataFrame): The DataFrame to analyze.\n",
      "    \"\"\"\n",
      "    # Use the built-in .info() method to display basic info\n",
      "    df.info(memory_usage='deep')\n",
      "    \n",
      "    # Calculate and print total memory usage\n",
      "    total_memory_usage = df.memory_usage(deep=True).sum() / (1)\n",
      "    print(f\"\\nTotal memory usage: {total_memory_usage:.2f} MB\")\n",
      "    \n",
      "    # Calculate and display memory usage per column\n",
      "    column_memory_usage = (df.memory_usage(deep=True) / (1)).round(2)\n",
      "    print(\"\\nMemory usage per column (B):\")\n",
      "    print(column_memory_usage)\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def generate_df(n):\n",
      "    \"\"\"\n",
      "    Generate a random DataFrame of size n x n.\n",
      "\n",
      "    Parameters:\n",
      "    n (int): The number of rows and columns for the DataFrame.\n",
      "\n",
      "    Returns:\n",
      "    pd.DataFrame: A DataFrame of size n x n filled with random values.\n",
      "    \"\"\"\n",
      "    np.random.seed(0)  # For reproducibility\n",
      "    data = np.random.rand(n, n)\n",
      "    return pd.DataFrame(data)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def dataframe():\n",
      "    # Get all variables in the current environment\n",
      "    all_vars = globals()\n",
      "    \n",
      "    # Filter out only those that are pandas DataFrames\n",
      "    dataframes = {name: var for name, var in all_vars.items() if isinstance(var, pd.DataFrame)}\n",
      "    \n",
      "    # Display the names and shapes of all DataFrames\n",
      "    for name, df in dataframes.items():\n",
      "        print(f\"{name}: {df.shape}\")\n",
      "\n",
      "# Example usage:\n",
      "# dataframe()\n",
      "import re\n",
      "\n",
      "def to_snake_case(text):\n",
      "    \"\"\"\n",
      "    Convert a string to snake_case.\n",
      "\n",
      "    Parameters:\n",
      "    - text (str): The input string to be converted.\n",
      "\n",
      "    Returns:\n",
      "    - str: The snake_case version of the input string.\n",
      "    \"\"\"\n",
      "    # Remove all dots and other special characters, convert spaces and hyphens to underscores, and lower case the entire string\n",
      "    text = re.sub(r'[^\\w\\s-]', '', text)  # Remove all non-word characters except spaces and hyphens\n",
      "    text = re.sub(r'[\\s-]+', '_', text)   # Replace spaces and hyphens with underscores\n",
      "    text = re.sub(r'_+', '_', text)       # Replace multiple underscores with a single underscore\n",
      "    \n",
      "    # Convert to lower case\n",
      "    return text.lower()\n",
      "\n",
      "# Example usage:\n",
      "# descriptor = to_snake_case(\"Pairwise Correlation Matrix. V1\")\n",
      "# print(descriptor)  # Output: \"pairwise_correlation_matrix_v1\"\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 8:\n",
      "\n",
      "listdir(data)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 9:\n",
      "\n",
      "os.listdir(chunks)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 10:\n",
      "\n",
      "os.listdir(data)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 11:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 12:\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "# Load the Parquet file into a DataFrame\n",
      "dublinbikes_df = pd.read_parquet('data/dublinbikes_clean.parquet')\n",
      "geography_df = pd.read_parquet('data/geography_clean.parquet')\n",
      "weather_df = pd.read_parquet('data/weather_transformed_clean_friday.parquet')\n",
      "capitalbikes_df = read_chunks('capitalbikes_clean_narrow')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 13:\n",
      "\n",
      "capitalbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 14:\n",
      "\n",
      "dublinbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 15:\n",
      "\n",
      "weather_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 16:\n",
      "\n",
      "geography_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 17:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 18:\n",
      "\n",
      "capitalbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 19:\n",
      "\n",
      "capitalbikes_df.info()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 20:\n",
      "\n",
      "capitalbikes_df.describe()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 21:\n",
      "\n",
      "\n",
      "# Step 1: Randomly select 114 unique start_station_id values\n",
      "unique_stations = capitalbikes_df['start_station_id'].unique()\n",
      "sample_stations = pd.Series(unique_stations).sample(n=114, random_state=1).tolist()\n",
      "\n",
      "# Step 2: Filter the DataFrame to include only rows with these selected start_station_id values\n",
      "capitalbikes_sample_df = capitalbikes_df[capitalbikes_df['start_station_id'].isin(sample_stations)]\n",
      "capitalbikes_sample_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 22:\n",
      "\n",
      "## sneaky swap\n",
      "capitalbikes_df,capitalbikes_sample_df=capitalbikes_sample_df,capitalbikes_df\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 23:\n",
      "\n",
      "capitalbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 24:\n",
      "\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Sample 10% of the data\n",
      "capitalbikes_temp_df = capitalbikes_df.sample(frac=0.1, random_state=42)\n",
      "\n",
      "# Generate the pair plot\n",
      "pairplot = sns.pairplot(capitalbikes_temp_df)\n",
      "pairplot.fig.suptitle('Capital Bikes USA Pairwise Plots (10% Sampled Data)', y=1.02)\n",
      "\n",
      "# Path to save the image\n",
      "image_path = 'images/Capital_Bikes_USA_Pairplots_Sampled_10pct.png'\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(image_path)\n",
      "\n",
      "# Display the newly generated plot\n",
      "plt.show()\n",
      "print(f\"Image '{image_path}' created and displayed.\")\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 25:\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Set the style to dark background with white grid lines\n",
      "sns.set_style(\"darkgrid\")\n",
      "\n",
      "# Plot distribution for 'distance_km'\n",
      "plt.figure(figsize=(12, 5))\n",
      "sns.histplot(capitalbikes_df['distance_km'], kde=True, color='blue', bins=30)\n",
      "plt.title('Capital Bikes USA Distribution of Distance (km)')\n",
      "plt.xlabel('Distance (km)')\n",
      "plt.ylabel('Frequency')\n",
      "# Save the figure\n",
      "plt.savefig('images/Capital_Bikes_USA_histplot-distance_km.png')\n",
      "plt.show()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 26:\n",
      "\n",
      "# Plot distribution for 'duration_min'\n",
      "plt.figure(figsize=(12, 5))\n",
      "sns.histplot(capitalbikes_df['duration_min'], kde=True, color='green', bins=30)\n",
      "plt.title('Capital Bikes USA Distribution of Duration (min)')\n",
      "plt.xlabel('Duration (min)')\n",
      "plt.ylabel('Frequency')\n",
      "# Save the figure\n",
      "plt.savefig('images/Capital_Bikes_USA_histplot-duration_min.png')\n",
      "plt.show()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 27:\n",
      "\n",
      "# Plot distribution for 'average_speed_kmh'\n",
      "plt.figure(figsize=(12, 5))\n",
      "sns.histplot(capitalbikes_df['average_speed_kmh'], kde=True, color='red', bins=30)\n",
      "plt.title('Capital Bikes USA Distribution of Average Speed (km/h)')\n",
      "plt.xlabel('Average Speed (km/h)')\n",
      "plt.ylabel('Frequency')\n",
      "# Save the figure\n",
      "plt.savefig('images/Capital_Bikes_USA_histplot-average_speed_kmh.png')\n",
      "plt.show()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 28:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 29:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Count occurrences of each start_station_id\n",
      "start_station_counts = capitalbikes_df['start_station_id'].value_counts()\n",
      "\n",
      "# Plotting the bar plot\n",
      "plt.figure(figsize=(12, 6))\n",
      "start_station_counts.plot(kind='bar')\n",
      "plt.xlabel('Start Station ID')\n",
      "plt.ylabel('Frequency')\n",
      "plt.title('Capital Bikes USA sample 114 Distribution of Start Station IDs')\n",
      "plt.xticks(rotation=90)\n",
      "# Save the figure\n",
      "plt.savefig('images/capital_bikes_usa_sample_distribution_of_start_station_ids.png') \n",
      "# Get the title and convert it to snake_case\n",
      "title = plt.gca().get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)\n",
      "\n",
      "# Call the function to save the figure with the given descriptor\n",
      "save_image(descriptor)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 30:\n",
      "\n",
      "save_files(capitalbikes_df,'capitalbikes_sample114_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 31:\n",
      "\n",
      "%%time\n",
      "\n",
      "# Extract 'time' and 'date' from the 'datetime' column\n",
      "capitalbikes_df['time'] = capitalbikes_df['datetime'].dt.time\n",
      "capitalbikes_df['date'] = capitalbikes_df['datetime'].dt.date\n",
      "\n",
      "# Determine the day of the week and classify the day type\n",
      "capitalbikes_df['day'] = capitalbikes_df['datetime'].dt.dayofweek\n",
      "capitalbikes_df['day_type'] = np.where(capitalbikes_df['day'] <= 4, 'weekday', \n",
      "                          np.where(capitalbikes_df['day'] == 5, 'saturday', 'sunday'))\n",
      "\n",
      "# Define a function to categorize the time of day into different bins\n",
      "def bin_time(x):\n",
      "    if x.time() < dt.time(6):\n",
      "        return \"Overnight\"\n",
      "    elif x.time() < dt.time(11):\n",
      "        return \"6AM-10AM\"\n",
      "    elif x.time() < dt.time(16):\n",
      "        return \"11AM-3PM\"\n",
      "    elif x.time() < dt.time(20):\n",
      "        return \"4PM-7PM\"\n",
      "    elif x.time() <= dt.time(23):\n",
      "        return \"8PM-11PM\"\n",
      "    else:\n",
      "        return \"Overnight\"\n",
      "\n",
      "# Apply the binning function to categorize the time of day\n",
      "capitalbikes_df[\"time_type\"] = capitalbikes_df['datetime'].apply(bin_time)\n",
      "\n",
      "# Extract the hour and month from the 'datetime' column\n",
      "capitalbikes_df['hour'] = capitalbikes_df['datetime'].dt.hour\n",
      "capitalbikes_df['month'] = capitalbikes_df['datetime'].dt.month\n",
      "\n",
      "# Create a cluster group by combining 'time_type' and 'day_type'\n",
      "capitalbikes_df['cluster_group'] = capitalbikes_df['time_type'] + \" \" + capitalbikes_df['day_type']\n",
      "\n",
      "# Display the shape of the DataFrame to verify processing\n",
      "capitalbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 32:\n",
      "\n",
      "# Define a function to map months to seasons\n",
      "def get_season(month):\n",
      "    if month in [12, 1, 2]:\n",
      "        return 'Winter'\n",
      "    elif month in [3, 4, 5]:\n",
      "        return 'Spring'\n",
      "    elif month in [6, 7, 8]:\n",
      "        return 'Summer'\n",
      "    else:\n",
      "        return 'Autumn'\n",
      "\n",
      "# Create a 'season' column based on the 'month'\n",
      "capitalbikes_df['season'] = capitalbikes_df['month'].apply(get_season)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 33:\n",
      "\n",
      "capitalbikes_df.season.unique()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 34:\n",
      "\n",
      "capitalbikes_df.nunique()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 35:\n",
      "\n",
      "capitalbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 36:\n",
      "\n",
      "%%time\n",
      "# Generate the list of U.S. holidays for the year 2023\n",
      "us_holidays = holidays.US(years=[2023])\n",
      "\n",
      "# Convert the holidays to a list of dates\n",
      "holiday_dates = pd.to_datetime(list(us_holidays.keys()))\n",
      "\n",
      "# Convert 'date' column to datetime64[ns] if it's not already in that format\n",
      "capitalbikes_df['date'] = pd.to_datetime(capitalbikes_df['date'])\n",
      "\n",
      "# Ensure 'holiday_dates' is in datetime64[ns] format\n",
      "holiday_dates = pd.to_datetime(holiday_dates)\n",
      "\n",
      "# Add 'is_holiday' column to the DataFrame\n",
      "capitalbikes_df['is_holiday'] = capitalbikes_df['date'].apply(lambda x: 1 if x in holiday_dates else 0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 37:\n",
      "\n",
      "capitalbikes_df[capitalbikes_df['is_holiday'] == 1].head(1)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 38:\n",
      "\n",
      "capitalbikes_df.dtypes\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 39:\n",
      "\n",
      "capitalbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 40:\n",
      "\n",
      "# Create a datetime_floor field in the original DataFrame\n",
      "capitalbikes_df['datetime_floor'] = pd.to_datetime(capitalbikes_df['datetime']).dt.floor('H')\n",
      "capitalbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 41:\n",
      "\n",
      "capitalbikes_df.dtypes\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 42:\n",
      "\n",
      "capitalbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 43:\n",
      "\n",
      "capitalbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 44:\n",
      "\n",
      "capitalbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 45:\n",
      "\n",
      "# Save the first 1,000,000 rows of the DataFrame to a CSV file\n",
      "capitalbikes_df.head(1000000).to_csv('data/capitalbikes_df_sample_1000000.csv', index=False)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 46:\n",
      "\n",
      "# Group by 'start_station_id', 'date', 'hour', and 'datetime_floor' to count the number of trips\n",
      "station_traffic_df = capitalbikes_df.groupby(['start_station_id', 'date', 'hour', 'datetime_floor']).size().reset_index(name='trip_count')\n",
      "station_traffic_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 47:\n",
      "\n",
      "station_traffic_df.sample(5)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 48:\n",
      "\n",
      "# Drop unwanted columns before merging\n",
      "capitalbikes_df_station_info = capitalbikes_df.drop(columns=['distance_km', 'duration_min', 'average_speed_kmh'])\n",
      "\n",
      "# Merge the grouped data with the station-specific info\n",
      "capitalbikes_station_df = station_traffic_df.merge(\n",
      "    capitalbikes_df_station_info,\n",
      "    on=['start_station_id', 'date', 'hour', 'datetime_floor'],\n",
      "    how='left'\n",
      ")\n",
      "\n",
      "# Display the first few rows of the resulting DataFrame\n",
      "capitalbikes_station_df.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 49:\n",
      "\n",
      "capitalbikes_station_df['trip_count'].unique()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 50:\n",
      "\n",
      "capitalbikes_station_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 51:\n",
      "\n",
      "save_files(capitalbikes_station_df,'capitalbikes_station_featured_dataframe_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 52:\n",
      "\n",
      "capitalbikes_station_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 53:\n",
      "\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "\n",
      "# Convert 'date' to datetime type if not already done\n",
      "capitalbikes_station_df['date'] = pd.to_datetime(capitalbikes_station_df['date'])\n",
      "\n",
      "# Create a 'weekday' column based on the 'date'\n",
      "capitalbikes_station_df['weekday'] = capitalbikes_station_df['date'].dt.day_name()\n",
      "\n",
      "# Aggregate the data by 'hour' and 'weekday' to calculate total trip count\n",
      "hourly_weekday_df = capitalbikes_station_df.groupby(['hour', 'weekday']).agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Sort the weekdays to ensure the plot is in the correct order\n",
      "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
      "\n",
      "# Plot the data\n",
      "fig, ax = plt.subplots(figsize=(20, 10))\n",
      "sns.pointplot(data=hourly_weekday_df, x='hour', y='trip_count', hue='weekday', hue_order=weekday_order, ax=ax)\n",
      "ax.set(title='Capital Bikes USA Count of Bikes During Weekdays and Weekends', xlabel='Hour of the Day', ylabel='Total Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "# Get the title and convert it to snake_case\n",
      "title = plt.gca().get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)\n",
      "\n",
      "# Call the function to save the figure with the given descriptor\n",
      "save_image(descriptor)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 54:\n",
      "\n",
      "# Aggregate the data by 'hour' and 'season' to calculate total trip count\n",
      "hourly_season_df = capitalbikes_station_df.groupby(['hour', 'season']).agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Plot the data\n",
      "fig, ax = plt.subplots(figsize=(20, 10))\n",
      "sns.pointplot(data=hourly_season_df, x='hour', y='trip_count', hue='season', ax=ax)\n",
      "ax.set(title='Capital Bikes USA Count of Bikes During Different Seasons', xlabel='Hour of the Day', ylabel='Total Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "# Get the title and convert it to snake_case\n",
      "title = plt.gca().get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)\n",
      "\n",
      "# Call the function to save the figure with the given descriptor\n",
      "save_image(descriptor)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 55:\n",
      "\n",
      "# Aggregate the data by 'weekday' to calculate total trip count\n",
      "weekday_df = capitalbikes_station_df.groupby('weekday').agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Sort the weekdays to ensure the plot is in the correct order\n",
      "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
      "\n",
      "# Plot the data\n",
      "fig, ax = plt.subplots(figsize=(20, 10))\n",
      "sns.barplot(data=weekday_df, x='weekday', y='trip_count', order=weekday_order, ax=ax)\n",
      "ax.set(title='Capital Bikes USA Count of Bikes During Different Days', xlabel='Weekday', ylabel='Total Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "# Save the figure before displaying it\n",
      "title = ax.get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)  # Convert the title to snake_case\n",
      "save_image(descriptor)  # Save the figure\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 56:\n",
      "\n",
      "# Aggregate the data by 'month' to calculate total trip count\n",
      "monthly_df = capitalbikes_station_df.groupby('month').agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Plot the data\n",
      "fig, ax = plt.subplots(figsize=(20, 10))\n",
      "sns.barplot(data=monthly_df, x='month', y='trip_count', ax=ax)\n",
      "ax.set(title='Capital Bikes USA Count of Bikes During Different Months', xlabel='Month', ylabel='Total Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "# Save the figure before displaying it\n",
      "title = ax.get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)  # Convert the title to snake_case\n",
      "save_image(descriptor)  # Save the figure\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 57:\n",
      "\n",
      "# Ensure 'date' is in datetime format\n",
      "capitalbikes_station_df['date'] = pd.to_datetime(capitalbikes_station_df['date'])\n",
      "\n",
      "# Filter the data for the year 2023\n",
      "capitalbikes_station_df_2023 = capitalbikes_station_df[capitalbikes_station_df['date'].dt.year == 2023]\n",
      "\n",
      "# Aggregate the data by 'date' to calculate total trip count across all stations\n",
      "daily_total_df = capitalbikes_station_df_2023.groupby('date').agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Plot the time series data\n",
      "plt.figure(figsize=(20, 10))\n",
      "sns.lineplot(data=daily_total_df, x='date', y='trip_count', marker='o')\n",
      "plt.title('Capital Bikes USA Daily Total Bike Trip Counts Across All Stations in 2023')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Total Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "# Save the image with the title as the filename\n",
      "title = plt.gca().get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)  # Convert the title to snake_case\n",
      "save_image(descriptor)  # Save the figure\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 58:\n",
      "\n",
      "# Define your custom save_image function\n",
      "\n",
      "# Ensure 'date' is in datetime format\n",
      "capitalbikes_station_df['date'] = pd.to_datetime(capitalbikes_station_df['date'])\n",
      "\n",
      "# Filter the data for the year 2023\n",
      "capitalbikes_station_df_2023 = capitalbikes_station_df[capitalbikes_station_df['date'].dt.year == 2023]\n",
      "\n",
      "# Aggregate the data by 'date' to calculate total trip count across all stations\n",
      "daily_total_df = capitalbikes_station_df_2023.groupby('date').agg({'trip_count': 'sum'}).reset_index()\n",
      "\n",
      "# Define the window size\n",
      "window_size = 21 # Change this value as needed\n",
      "\n",
      "# Calculate the rolling window average\n",
      "daily_total_df[f'rolling_{window_size}d_avg'] = daily_total_df['trip_count'].rolling(window=window_size, min_periods=1).mean()\n",
      "\n",
      "# Plot the time series data with rolling window average\n",
      "plt.figure(figsize=(20, 10))\n",
      "sns.lineplot(data=daily_total_df, x='date', y=f'rolling_{window_size}d_avg', marker='o')\n",
      "plot_title = f'{window_size}- Capital Bikes USA  Day Rolling Average of Daily Bike Trip Counts Across All Stations in 2023'\n",
      "plt.title(plot_title)\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel(f'{window_size}-Day Rolling Average of Trip Count')\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image(f\"{to_snake_case(plot_title)}.png\")\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 59:\n",
      "\n",
      "image_index\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 60:\n",
      "\n",
      "capitalbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 61:\n",
      "\n",
      "station_traffic_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 62:\n",
      "\n",
      "station_traffic_df\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 63:\n",
      "\n",
      "# Call the save_files function\n",
      "save_files(station_traffic_df, 'station_traffic')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 64:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 65:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Calculate descriptive statistics\n",
      "average_speed_stats = capitalbikes_df['average_speed_kmh'].describe()\n",
      "\n",
      "# Additional statistics\n",
      "median_speed = capitalbikes_df['average_speed_kmh'].median()\n",
      "std_dev_speed = capitalbikes_df['average_speed_kmh'].std()\n",
      "percentiles = capitalbikes_df['average_speed_kmh'].quantile([0.25, 0.5, 0.75])\n",
      "\n",
      "print(\"Descriptive Statistics:\")\n",
      "print(average_speed_stats)\n",
      "print(\"\\nAdditional Statistics:\")\n",
      "print(f\"Median Speed: {median_speed}\")\n",
      "print(f\"Standard Deviation: {std_dev_speed}\")\n",
      "print(f\"Percentiles:\\n{percentiles}\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 66:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "# Calculate the mean, standard error, and 95% confidence interval for the trip_count\n",
      "mean_trip_count = np.mean(station_traffic_df['trip_count'])\n",
      "sem_trip_count = stats.sem(station_traffic_df['trip_count'])\n",
      "\n",
      "# 95% confidence interval for the mean trip count\n",
      "confidence_interval = stats.t.interval(\n",
      "    0.95, \n",
      "    len(station_traffic_df['trip_count'])-1, \n",
      "    loc=mean_trip_count, \n",
      "    scale=sem_trip_count\n",
      ")\n",
      "\n",
      "# Hypothesis Testing: Test if the mean trip count is significantly different from 2\n",
      "hypothesized_mean = 2\n",
      "t_statistic, p_value = stats.ttest_1samp(station_traffic_df['trip_count'], hypothesized_mean)\n",
      "\n",
      "mean_trip_count, confidence_interval, t_statistic, p_value\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 67:\n",
      "\n",
      "import numpy as np\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Calculate the mean, standard error, and 95% confidence interval for the trip_count in capitalbikes_station_df\n",
      "mean_trip_count = np.mean(capitalbikes_station_df['trip_count'])\n",
      "sem_trip_count = stats.sem(capitalbikes_station_df['trip_count'])\n",
      "\n",
      "# 95% confidence interval for the mean trip count\n",
      "confidence_interval = stats.t.interval(\n",
      "    0.95, \n",
      "    len(capitalbikes_station_df['trip_count'])-1, \n",
      "    loc=mean_trip_count, \n",
      "    scale=sem_trip_count\n",
      ")\n",
      "\n",
      "# Hypothesis Testing: Test if the mean trip count is significantly different from 2\n",
      "hypothesized_mean = 2\n",
      "t_statistic, p_value = stats.ttest_1samp(capitalbikes_station_df['trip_count'], hypothesized_mean)\n",
      "\n",
      "# Print the results\n",
      "print(\"Mean Trip Count:\", mean_trip_count)\n",
      "print(\"95% Confidence Interval:\", confidence_interval)\n",
      "print(\"t-Statistic:\", t_statistic)\n",
      "print(\"p-Value:\", p_value)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 68:\n",
      "\n",
      "listdir(data)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 69:\n",
      "\n",
      "\n",
      "\n",
      "# Display the first few rows to confirm it loaded correctly\n",
      "weather_df.sample(9)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 70:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 71:\n",
      "\n",
      "### Prepare Weather Data further\n",
      "# Add binary variables for dry and warm\n",
      "weather_df['dry'] = np.where(weather_df['rain'] == 0.0, 1, 0)  # 1 for dry (no rain), 0 for not dry (rain present)\n",
      "weather_df['warm'] = np.where(weather_df['temp'] > 18.0, 1, 0)  # 1 for warm (temperature > 18.0C), 0 for not warm\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 72:\n",
      "\n",
      "weather_df.sample(5)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 73:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 74:\n",
      "\n",
      "listdir(chunks)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 75:\n",
      "\n",
      "# Filter for a specific station_id, for example, station_id = 2\n",
      "station_id = 2\n",
      "station_data = dublinbikes_df[dublinbikes_df['station_id'] == station_id]\n",
      "\n",
      "# Get the first 100 rows for this station_id\n",
      "first_100_rows = station_data.head(100)\n",
      "\n",
      "# Display the first few rows to check the result\n",
      "first_100_rows.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 76:\n",
      "\n",
      "# Assuming `dublinbikes_df` is your DataFrame after reading the chunks\n",
      "dublinbikes_df_grouped = dublinbikes_df.groupby('station_id')\n",
      "\n",
      "# If you want to see the first few rows for each group, you can do:\n",
      "dublinbikes_df_head = dublinbikes_df_grouped.head()\n",
      "\n",
      "# Display the head of the grouped DataFrame\n",
      "dublinbikes_df_head.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 77:\n",
      "\n",
      "dublinbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 78:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 79:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime as dt\n",
      "\n",
      "\n",
      "\n",
      "# Convert 'last_updated' column to datetime format\n",
      "dublinbikes_df['last_updated'] = pd.to_datetime(dublinbikes_df['last_updated'])\n",
      "\n",
      "# Extract 'time' and 'date' from the 'last_updated' column\n",
      "dublinbikes_df['time'] = dublinbikes_df['last_updated'].dt.time\n",
      "dublinbikes_df['date'] = dublinbikes_df['last_updated'].dt.date\n",
      "\n",
      "# Calculate occupancy percentage\n",
      "dublinbikes_df['occupancy_pct'] = dublinbikes_df['available_bikes'] / (dublinbikes_df['available_bike_stands'] + dublinbikes_df['available_bikes'])\n",
      "\n",
      "# Identify full and empty stations\n",
      "dublinbikes_df['full'] = np.where(dublinbikes_df['occupancy_pct'] == 1, 1, 0)\n",
      "dublinbikes_df['empty'] = np.where(dublinbikes_df['occupancy_pct'] == 0, 1, 0)\n",
      "\n",
      "# Determine the day of the week and classify the day type\n",
      "dublinbikes_df['day_number'] = dublinbikes_df['last_updated'].dt.dayofweek\n",
      "dublinbikes_df['day_type'] = np.where(dublinbikes_df['day_number'] <= 4, 'weekday', \n",
      "                          np.where(dublinbikes_df['day_number'] == 5, 'saturday', 'sunday'))\n",
      "\n",
      "# Define a function to categorize the time of day into different bins\n",
      "def bin_time(x):\n",
      "    if x.time() < dt.time(6):\n",
      "        return \"Overnight\"\n",
      "    elif x.time() < dt.time(11):\n",
      "        return \"6AM-10AM\"\n",
      "    elif x.time() < dt.time(16):\n",
      "        return \"11AM-3PM\"\n",
      "    elif x.time() < dt.time(20):\n",
      "        return \"4PM-7PM\"\n",
      "    elif x.time() <= dt.time(23):\n",
      "        return \"8PM-11PM\"\n",
      "    else:\n",
      "        return \"Overnight\"\n",
      "\n",
      "# Apply the binning function to categorize the time of day\n",
      "dublinbikes_df[\"time_type\"] = dublinbikes_df['last_updated'].apply(bin_time)\n",
      "\n",
      "# Extract the hour and month from the 'last_updated' column\n",
      "dublinbikes_df['hour'] = dublinbikes_df['last_updated'].dt.hour\n",
      "dublinbikes_df['month'] = dublinbikes_df['last_updated'].dt.month\n",
      "dublinbikes_df['year'] = dublinbikes_df['last_updated'].dt.year\n",
      "\n",
      "# Create a cluster group by combining 'time_type' and 'day_type'\n",
      "dublinbikes_df['cluster_group'] = dublinbikes_df['time_type'] + \" \" + dublinbikes_df['day_type']\n",
      "\n",
      "# Identify bike arrivals and bike departures\n",
      "dublinbikes_df['bike_arr_dep'] = dublinbikes_df.groupby('station_id')['available_bike_stands'].diff(-1)\n",
      "dublinbikes_df['bike_arr'] = np.where(dublinbikes_df['bike_arr_dep'] > 0, dublinbikes_df['bike_arr_dep'], 0)\n",
      "dublinbikes_df['bike_dep'] = np.where(dublinbikes_df['bike_arr_dep'] < 0, dublinbikes_df['bike_arr_dep'], 0)\n",
      "dublinbikes_df['activity_type'] = np.where(abs(dublinbikes_df['bike_arr_dep']) >= 10, \"rebalancing\", \"rental\")\n",
      "dublinbikes_df['imbalanced'] = np.where(dublinbikes_df['occupancy_pct'] < 0.1, 1, \n",
      "                            np.where(dublinbikes_df['occupancy_pct'] > 0.9, 1, 0))\n",
      "\n",
      "# Identify days with rebalancing\n",
      "dublinbikes_df['rebalancing'] = np.where(dublinbikes_df['activity_type'] == 'rebalancing', 1, 0)\n",
      "dublinbikes_df['join_on'] = dublinbikes_df['station_id'].astype(str) + dublinbikes_df['date'].astype(str)\n",
      "\n",
      "# Group by 'join_on' to sum up rebalancing activities per station per day\n",
      "join_table = dublinbikes_df.groupby(['join_on'])['rebalancing'].sum()\n",
      "\n",
      "# Convert join_table to a DataFrame and reset the index\n",
      "join_table = join_table.to_frame().reset_index()\n",
      "\n",
      "# Merge the join_table back with the original DataFrame\n",
      "dublinbikes_df = pd.merge(dublinbikes_df, join_table, on='join_on', how='left')\n",
      "\n",
      "# Drop the unnecessary columns\n",
      "dublinbikes_df = dublinbikes_df.drop(['join_on', 'rebalancing_x'], axis=1)\n",
      "\n",
      "# Renaming the merged column for clarity\n",
      "dublinbikes_df.rename(columns={'rebalancing_y': 'rebalancing'}, inplace=True)\n",
      "\n",
      "# Define seasons based on the month\n",
      "def get_season(month):\n",
      "    if month in [12, 1, 2]:\n",
      "        return 'Winter'\n",
      "    elif month in [3, 4, 5]:\n",
      "        return 'Spring'\n",
      "    elif month in [6, 7, 8]:\n",
      "        return 'Summer'\n",
      "    else:\n",
      "        return 'Autumn'\n",
      "\n",
      "# Apply the season categorization\n",
      "dublinbikes_df['season'] = dublinbikes_df['month'].apply(get_season)\n",
      "\n",
      "# Display the first few rows to verify the processing\n",
      "dublinbikes_df.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 80:\n",
      "\n",
      "## feature engineering  dublinbikes\n",
      "# Extract 'time' and 'date' from the 'last_updated' column\n",
      "dublinbikes_df['time'] = dublinbikes_df['last_updated'].dt.time\n",
      "dublinbikes_df['date'] = dublinbikes_df['last_updated'].dt.date\n",
      "\n",
      "# Calculate occupancy percentage\n",
      "dublinbikes_df['occupancy_pct'] = dublinbikes_df['available_bikes'] / (dublinbikes_df['available_bike_stands'] + dublinbikes_df['available_bikes'])\n",
      "\n",
      "# Identify full and empty stations\n",
      "dublinbikes_df['full'] = np.where(dublinbikes_df['occupancy_pct'] == 1, 1, 0)\n",
      "dublinbikes_df['empty'] = np.where(dublinbikes_df['occupancy_pct'] == 0, 1, 0)\n",
      "\n",
      "# Determine the day of the week and classify the day type\n",
      "dublinbikes_df['day_number'] = dublinbikes_df['last_updated'].dt.dayofweek\n",
      "dublinbikes_df['day_type'] = np.where(dublinbikes_df['day_number'] <= 4, 'weekday', \n",
      "                          np.where(dublinbikes_df['day_number'] == 5, 'saturday', 'sunday'))\n",
      "\n",
      "# Define a function to categorize the time of day into different bins\n",
      "def bin_time(x):\n",
      "    if x.time() < dt.time(6):\n",
      "        return \"Overnight\"\n",
      "    elif x.time() < dt.time(11):\n",
      "        return \"6AM-10AM\"\n",
      "    elif x.time() < dt.time(16):\n",
      "        return \"11AM-3PM\"\n",
      "    elif x.time() < dt.time(20):\n",
      "        return \"4PM-7PM\"\n",
      "    elif x.time() <= dt.time(23):\n",
      "        return \"8PM-11PM\"\n",
      "    else:\n",
      "        return \"Overnight\"\n",
      "\n",
      "# Apply the binning function to categorize the time of day\n",
      "dublinbikes_df[\"time_type\"] = dublinbikes_df['last_updated'].apply(bin_time)\n",
      "\n",
      "# Extract the hour and month from the 'last_updated' column\n",
      "dublinbikes_df['hour'] = dublinbikes_df['last_updated'].dt.hour\n",
      "dublinbikes_df['month'] = dublinbikes_df['last_updated'].dt.month\n",
      "dublinbikes_df['year'] = dublinbikes_df['last_updated'].dt.year\n",
      "\n",
      "# Create a cluster group by combining 'time_type' and 'day_type'\n",
      "dublinbikes_df['cluster_group'] = dublinbikes_df['time_type'] + \" \" + dublinbikes_df['day_type']\n",
      "\n",
      "# Display the shape of the DataFrame to verify processing\n",
      "print(dublinbikes_df.shape)\n",
      "\n",
      "\n",
      "# Identify bike arrivals and bike departures\n",
      "dublinbikes_df['bike_arr_dep'] = dublinbikes_df.groupby('station_id')['available_bike_stands'].diff(-1)\n",
      "dublinbikes_df['bike_arr'] = np.where(dublinbikes_df['bike_arr_dep'] > 0, dublinbikes_df['bike_arr_dep'], 0)\n",
      "dublinbikes_df['bike_dep'] = np.where(dublinbikes_df['bike_arr_dep'] < 0, dublinbikes_df['bike_arr_dep'], 0)\n",
      "dublinbikes_df['activity_type'] = np.where(abs(dublinbikes_df['bike_arr_dep']) >= 10, \"rebalancing\", \"rental\")\n",
      "dublinbikes_df['imbalanced'] = np.where(dublinbikes_df['occupancy_pct'] < 0.1, 1, \n",
      "                            np.where(dublinbikes_df['occupancy_pct'] > 0.9, 1, 0))\n",
      "\n",
      "# Identify days with rebalancing\n",
      "dublinbikes_df['rebalancing'] = np.where(dublinbikes_df['activity_type'] == 'rebalancing', 1, 0)\n",
      "dublinbikes_df['join_on'] = dublinbikes_df['station_id'].astype(str) + dublinbikes_df['date'].astype(str)\n",
      "\n",
      "# Group by 'join_on' to sum up rebalancing activities per station per day\n",
      "join_table = dublinbikes_df.groupby(['join_on'])['rebalancing'].sum()\n",
      "\n",
      "# Convert join_table to a DataFrame and reset the index\n",
      "join_table = join_table.to_frame().reset_index()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Merge the join_table back with the original DataFrame\n",
      "dublinbikes_df = pd.merge(dublinbikes_df, join_table, on='join_on', how='left')\n",
      "\n",
      "# Drop the unnecessary columns\n",
      "dublinbikes_df = dublinbikes_df.drop(['join_on', 'rebalancing_x'], axis=1)\n",
      "\n",
      "# Renaming the merged column for clarity\n",
      "dublinbikes_df.rename(columns={'rebalancing_y': 'rebalancing'}, inplace=True)\n",
      "\n",
      "\n",
      "# Define seasons based on the month\n",
      "def get_season(month):\n",
      "    if month in [12, 1, 2]:\n",
      "        return 'Winter'\n",
      "    elif month in [3, 4, 5]:\n",
      "        return 'Spring'\n",
      "    elif month in [6, 7, 8]:\n",
      "        return 'Summer'\n",
      "    else:\n",
      "        return 'Autumn'\n",
      "\n",
      "# Apply the season categorization\n",
      "dublinbikes_df['season'] = dublinbikes_df['month'].apply(get_season)\n",
      "\n",
      "# Initialize Irish holidays\n",
      "irish_holidays = holidays.Ireland()\n",
      "\n",
      "# Ensure 'date' is of last_updated.date type\n",
      "dublinbikes_df['date'] = pd.to_datetime(dublinbikes_df['date']).dt.date\n",
      "\n",
      "# Create a new column 'holiday' that indicates if a date is a holiday\n",
      "dublinbikes_df['holiday'] = dublinbikes_df['date'].apply(lambda x: 1 if x in irish_holidays else 0)\n",
      "dublinbikes_df['join_on'] = dublinbikes_df['station_id'].astype(str) + '_' + dublinbikes_df['date'].astype(str)\n",
      "# Merge the join_table back with the original DataFrame\n",
      "dublinbikes_df = pd.merge(dublinbikes_df, join_table, on='join_on', how='left')\n",
      "\n",
      "# Drop the unnecessary columns\n",
      "dublinbikes_df = dublinbikes_df.drop(['join_on', 'rebalancing_x'], axis=1)\n",
      "\n",
      "# Renaming the merged column for clarity\n",
      "dublinbikes_df.rename(columns={'rebalancing_y': 'rebalancing'}, inplace=True)\n",
      "\n",
      "\n",
      "# Define seasons based on the month\n",
      "def get_season(month):\n",
      "    if month in [12, 1, 2]:\n",
      "        return 'Winter'\n",
      "    elif month in [3, 4, 5]:\n",
      "        return 'Spring'\n",
      "    elif month in [6, 7, 8]:\n",
      "        return 'Summer'\n",
      "    else:\n",
      "        return 'Autumn'\n",
      "\n",
      "# Apply the season categorization\n",
      "dublinbikes_df['season'] = dublinbikes_df['month'].apply(get_season)\n",
      "\n",
      "# Initialize Irish holidays\n",
      "irish_holidays = holidays.Ireland()\n",
      "\n",
      "# Ensure 'date' is of datetime.date type\n",
      "dublinbikes_df['date'] = pd.to_datetime(dublinbikes_df['date']).dt.date\n",
      "\n",
      "# Create a new column 'is_holiday' that indicates if a date is a holiday\n",
      "dublinbikes_df['holiday'] = dublinbikes_df['date'].apply(lambda x: 1 if x in irish_holidays else 0)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 81:\n",
      "\n",
      "dublinbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 82:\n",
      "\n",
      "dublinbikes_df.info()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 83:\n",
      "\n",
      "weather_df.info()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 84:\n",
      "\n",
      "save_files(dublinbikes_df,'dublinbikes_time_prepared_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 85:\n",
      "\n",
      "## file above 100MB removed and chunking used\n",
      "os.remove('data/dublinbikes_time_prepared_friday.csv')\n",
      "save_chunks(dublinbikes_df,'dublinbikes_time_prepared_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 86:\n",
      "\n",
      "dublinbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 87:\n",
      "\n",
      "dublinbikes_df['trip_count'] = (abs(dublinbikes_df['bike_arr']) + abs(dublinbikes_df['bike_dep'])) / 2\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 88:\n",
      "\n",
      "\n",
      "dublinbikes_df['trip_count'].nunique()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 89:\n",
      "\n",
      "\n",
      "\n",
      "# Calculate trip count as the average of bike arrivals and departures\n",
      "dublinbikes_df['trip_count'] = (abs(dublinbikes_df['bike_arr']) + abs(dublinbikes_df['bike_dep'])) / 2\n",
      "\n",
      "# Plot histogram\n",
      "plt.hist(dublinbikes_df['trip_count'], bins=30, edgecolor='k')\n",
      "plt.xlabel('trip_count')\n",
      "plt.ylabel('Frequency')\n",
      "plt.title('Distribution of Trip Count in Dublin Bikes Data')\n",
      "\n",
      "# Save the plot\n",
      "plt.savefig(\"data/distribution_of_trip_count_in_dublin_bikes_data.png\")\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 90:\n",
      "\n",
      "dublinbikes_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 91:\n",
      "\n",
      "# Rename the 'last_updated' column to 'datetime'\n",
      "dublinbikes_df.rename(columns={'last_updated': 'datetime'}, inplace=True)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 92:\n",
      "\n",
      "# Check the dates of the missing values in 'bike_arr_dep'\n",
      "# as expected these are last row for each station in dataframe\n",
      "missing_dates = dublinbikes_df[dublinbikes_df['bike_arr_dep'].isnull()][['station_id', 'datetime']]\n",
      "# Display the first few rows of missing dates\n",
      "missing_dates.head(3)\n",
      "# as expected last row per station\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 93:\n",
      "\n",
      "dublinbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 94:\n",
      "\n",
      "# Dropping rows where 'bike_arr_dep' has missing values\n",
      "# Note Rebalincing plan did not work but not essential for progress\n",
      "dublinbikes_df.dropna(subset=['bike_arr_dep'], inplace=True)\n",
      "dublinbikes_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 95:\n",
      "\n",
      "dublinbikes_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 96:\n",
      "\n",
      "save_chunks(dublinbikes_df,'dublinbikes_trip_counts_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 97:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 98:\n",
      "\n",
      "dublinbikes_df=read_chunks('dublinbikes_trip_counts_friday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 99:\n",
      "\n",
      "geography_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 100:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 101:\n",
      "\n",
      "dublinbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 102:\n",
      "\n",
      "dublinbikes_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 103:\n",
      "\n",
      "geography_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 104:\n",
      "\n",
      "geography_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 105:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 106:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 107:\n",
      "\n",
      "# Merging the dublinbikes_df dataframe with the geography dataframe on 'station_id'\n",
      "dublinbikes_geography_merge_df = dublinbikes_df.merge(geography_df, on='station_id', how='left')\n",
      "dublinbikes_geography_merge_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 108:\n",
      "\n",
      "dublinbikes_geography_merge_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 109:\n",
      "\n",
      "weather_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 110:\n",
      "\n",
      "# Merging the bg dataframe with the weather_df dataframe on 'datetime_floor' using a left join.\n",
      "# This will add the weather-related columns from weather_df to bgw, where rows are matched based on 'datetime_floor'.\n",
      "# Rows in bg that do not have a matching 'datetime_floor' in weather_df will have NaN values in the new columns from weather_df.\n",
      "dublinbikes_geography_weather_merge_df = dublinbikes_geography_merge_df.merge(weather_df, on='datetime_floor', how='left')\n",
      "dublinbikes_geography_weather_merge_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 111:\n",
      "\n",
      "save_chunks(dublinbikes_geography_weather_merge_df,'dublinbikes_geography_weather_merge')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 112:\n",
      "\n",
      "dublinbikes_geography_weather_merge_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 113:\n",
      "\n",
      "# Creating a clustering_df with subset columns\n",
      "clustering_df = dublinbikes_geography_weather_merge_df[['station_id', 'station_name', 'latitude', 'longitude', 'day_type', 'time_type', 'occupancy_pct', 'cluster_group']]\n",
      "\n",
      "# Display the first few rows of clustering_df to verify\n",
      "clustering_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 114:\n",
      "\n",
      "## Import Seaborn\n",
      "import seaborn as sns\n",
      "\n",
      "## Create a histogram of the occupancy_pct column\n",
      "sns.histplot(clustering_df['occupancy_pct'], bins=30, kde=False)\n",
      "title = ('occupancy_pct_dist')\n",
      "# Call the function to save the figure with the given descriptor\n",
      "save_image('occupancy_pct_dist')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 115:\n",
      "\n",
      "clustering_df.dtypes\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 116:\n",
      "\n",
      "clustering_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 117:\n",
      "\n",
      "# there are only 114 unique station names, it is indeed a good idea to\n",
      "# set station_name as a categorical variable\n",
      "\n",
      "clustering_df['station_name'] = clustering_df['station_name'].astype('category')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 118:\n",
      "\n",
      "clustering_df = clustering_df.drop(columns=['station_name', 'latitude', 'longitude'])\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 119:\n",
      "\n",
      "clustering_df.dtypes\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 120:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 121:\n",
      "\n",
      "# Grouping the 'clustering_df' dataframe by 'station_id', 'station_name', 'latitude', 'longitude', and 'cluster_group'.\n",
      "# For each unique combination of these columns, the mean of 'occupancy_pct' is calculated.\n",
      "# The result is a new dataframe where each group is represented by the average 'occupancy_pct'.\n",
      "clustering_df = clustering_df.groupby(\n",
      "    ['station_id',  'cluster_group'],  # Group by these columns\n",
      "    as_index=False  # Ensure the grouped columns remain as columns in the resulting dataframe rather than being set as the index\n",
      ")['occupancy_pct'].mean()  # Calculate the mean of 'occupancy_pct' for each group\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 122:\n",
      "\n",
      "save_files(clustering_df,'clustering')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 123:\n",
      "\n",
      "clustering_df\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 124:\n",
      "\n",
      "114*4*2*365\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 125:\n",
      "\n",
      "clustering_df = clustering_df.groupby(\n",
      "    ['station_id',  'cluster_group'],\n",
      "    as_index=False)['occupancy_pct'].mean()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 126:\n",
      "\n",
      "clustering_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 127:\n",
      "\n",
      "clustering_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 128:\n",
      "\n",
      "clustering_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 129:\n",
      "\n",
      "clustering_df  = clustering_df.set_index('station_id')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 130:\n",
      "\n",
      "save_files(clustering_df,'clustering_df_1710')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 131:\n",
      "\n",
      "# Pivot the 'clustering_df' dataframe to reorganize the data so that each 'cluster_group' becomes a separate column.\n",
      "# The 'occupancy_pct' values are spread across these columns for each unique combination of 'station_name', 'station_id', 'latitude', and 'longitude'.\n",
      "clustering_df_pivoted = clustering_df.pivot_table(\n",
      "    index=['station_id'],  # Use these columns to uniquely identify each station\n",
      "    columns=['cluster_group'],  # Each unique cluster group becomes a separate column\n",
      "    values='occupancy_pct'  # The values in these new columns will be the occupancy percentages\n",
      ")\n",
      "\n",
      "# Reset the index to convert the multi-level index created by the pivot operation back into regular columns, flattening the DataFrame.\n",
      "clustering_df_pivoted = clustering_df_pivoted.reset_index()\n",
      "\n",
      "# Set 'station_name' as the new index to organize the dataframe by station name.\n",
      "clustering_df_pivoted = clustering_df_pivoted.set_index('station_id')\n",
      "\n",
      "# Drop any rows with NaN values, which might occur if some stations do not have occupancy data for certain cluster groups.\n",
      "clustering_df_pivoted = clustering_df_pivoted.dropna()\n",
      "\n",
      "# Randomly sample 5 rows from the resulting dataframe to inspect a subset of the data.\n",
      "clustering_df_pivoted.sample(5)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 132:\n",
      "\n",
      "clustering_df_pivoted.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 133:\n",
      "\n",
      "save_files(clustering_df_pivoted,'clustering_df_pivoted')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 134:\n",
      "\n",
      "\n",
      "# Assuming geography_df has been loaded and contains 'station_id' as well as other geographic information\n",
      "# But we do not want to include 'station_address' in the merge\n",
      "\n",
      "# Specify the columns to include from geography_df, excluding 'station_address'\n",
      "columns_to_include = ['station_id', 'latitude', 'longitude', 'station_name']  # Add other columns as needed\n",
      "\n",
      "# Merge the dataframes on 'station_id' while excluding 'station_address'\n",
      "merged_df = pd.merge(clustering_df_pivoted, geography_df[columns_to_include], on='station_id', how='left')\n",
      "\n",
      "# Display the first few rows of the merged dataframe to verify\n",
      "merged_df.head()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 135:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Prepare the data for clustering by dropping non-numeric columns\n",
      "X = np.array(merged_df.drop(['station_id', 'latitude', 'longitude','station_name'], axis=1).astype(float))\n",
      "\n",
      "# Elbow method to determine the optimal number of clusters\n",
      "distortions = []\n",
      "K = range(1, 10)\n",
      "for k in K:\n",
      "    kmeanModel = KMeans(n_clusters=k)\n",
      "    kmeanModel.fit(X)\n",
      "    distortions.append(kmeanModel.inertia_)\n",
      "\n",
      "# Plot the distortions to find the elbow point\n",
      "plt.figure(figsize=(10, 7))\n",
      "plt.plot(K, distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal k')\n",
      "\n",
      "# Get the title and convert it to snake_case\n",
      "title = plt.gca().get_title()  # Retrieve the title from the current axes\n",
      "descriptor = to_snake_case(title)\n",
      "\n",
      "# Call the function to save the figure with the given descriptor\n",
      "save_image(descriptor)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 136:\n",
      "\n",
      "locations.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 137:\n",
      "\n",
      "\n",
      "# Perform a left join on 'station_id' to bring back in station_name and coordinates\n",
      "locations_geography_df = pd.merge(locations, geography_df, on='station_id', how='left')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 138:\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "import numpy as np\n",
      "\n",
      "# # Prepare the data for clustering by dropping non-numeric columns\n",
      "# X = np.array(clustering_df_pivoted.drop(['station_id', 'latitude', 'longitude'], axis=1).astype(float))\n",
      "\n",
      "# Perform KMeans clustering\n",
      "KM = KMeans(n_clusters=5) \n",
      "KM.fit(X)\n",
      "clusters = KM.predict(X)\n",
      "\n",
      "# Add the cluster labels to the original dataframe\n",
      "locations = clustering_df_pivoted.copy()\n",
      "locations['cluster'] = clusters\n",
      "locations = locations.reset_index()\n",
      "\n",
      "# Display the first 5 rows of the dataframe with cluster labels\n",
      "locations.head(5)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 139:\n",
      "\n",
      "import folium\n",
      "\n",
      "# Cluster info dictionary\n",
      "cluster_info = {\n",
      "    0: {'color': 'red', 'description': 'City Centre'},\n",
      "    1: {'color': 'blue', 'description': 'Docklands & South City'},\n",
      "    2: {'color': 'saddlebrown', 'description': 'Transport Hubs'},\n",
      "    3: {'color': 'darkgreen', 'description': 'Outer Suburbs'},\n",
      "    4: {'color': 'darkmagenta', 'description': 'Transition Zone'},\n",
      "}\n",
      "\n",
      "# Create a map centered on Dublin\n",
      "dublin_map = folium.Map(location=[53.345, -6.2650], zoom_start=13.5)\n",
      "\n",
      "# Add circle markers to the map for each location, colored by cluster and with station name popup\n",
      "for _, row in locations_geography_df.iterrows():\n",
      "    LATITUDE = row['latitude']\n",
      "    LONGITUDE = row['longitude']\n",
      "    Cluster = row['cluster']\n",
      "    StationName = row['station_name']\n",
      "    \n",
      "    cluster_data = cluster_info[Cluster]\n",
      "    folium.CircleMarker(\n",
      "        location=[LATITUDE, LONGITUDE],\n",
      "        color=cluster_data['color'],  # Border color based on the cluster\n",
      "        radius=8,\n",
      "        fill_color=cluster_data['color'],  # Fill color based on the cluster\n",
      "        fill=True,\n",
      "        fill_opacity=0.9,\n",
      "        popup=StationName  # Add popup with the station name\n",
      "    ).add_to(dublin_map)\n",
      "\n",
      "# Create HTML for the legend\n",
      "legend_html = '''\n",
      "    <div style=\"position: fixed; \n",
      "                bottom: 50px; right: 50px; width: 150px; height: auto; \n",
      "                border:2px solid grey; background-color: white; \n",
      "                z-index:9999; font-size:14px;\n",
      "                padding: 10px;\">\n",
      "        <b>Cluster Key</b><br>\n",
      "        {0}\n",
      "    </div>\n",
      "'''.format(\n",
      "    ''.join(f'<i class=\"fa fa-circle\" style=\"color:{info[\"color\"]};\"></i> {info[\"description\"]}<br>'\n",
      "            for info in cluster_info.values())\n",
      ")\n",
      "\n",
      "# Add legend to the map in the lower right corner\n",
      "folium.Marker(\n",
      "    location=[53.33, -6.2653],  # Dummy location for the legend marker\n",
      "    icon=folium.DivIcon(html=legend_html)\n",
      ").add_to(dublin_map)\n",
      "\n",
      "# Display the map\n",
      "dublin_map\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 140:\n",
      "\n",
      "save_files(locations,'locations')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 141:\n",
      "\n",
      "save_files(locations_geography_df,'locations_geography_saturday')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 142:\n",
      "\n",
      "os.listdir(data)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 143:\n",
      "\n",
      "dublinbikes_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 144:\n",
      "\n",
      "# load main merged dataset from cleaning\n",
      "dublinbikes_weather_geography_df= pd.read_parquet('data/dublinbikes_weather_geography_merge_transformed_friday.parquet')\n",
      "merged_with_clusters_df = dublinbikes_weather_geography_df.copy()\n",
      "dublinbikes_weather_geography_df\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 145:\n",
      "\n",
      "# Use 'locations' for cluster information and lowercase the headers for consistency\n",
      "locations.columns = locations.columns.str.lower()\n",
      "cluster_output = locations[['station_id', 'cluster']].drop_duplicates(keep='first')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 146:\n",
      "\n",
      "# Merge the main dataset with the cluster information based on 'station_id'\n",
      "dublinbikes_weather_geography_df = pd.merge(dublinbikes_geography_weather_merge_df, cluster_output, on='station_id', how='left')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 147:\n",
      "\n",
      "dublinbikes_weather_geography_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 148:\n",
      "\n",
      "dublinbikes_geography_weather_merge_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 149:\n",
      "\n",
      "locations\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 150:\n",
      "\n",
      "# Select only the 'station_id' and 'cluster' columns\n",
      "cluster_station_id_df = locations[['station_id', 'cluster']]\n",
      "\n",
      "# Set 'station_id' as the index\n",
      "cluster_station_id_df.set_index('station_id', inplace=True)\n",
      "cluster_station_id_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 151:\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "# Assuming you already have your DataFrames 'dublinbikes_geography_weather_merge_df' and 'locations'\n",
      "\n",
      "# Perform a left join on 'station_id'\n",
      "prepared_df = pd.merge(dublinbikes_geography_weather_merge_df, cluster_station_id_df, on='station_id', how='left')\n",
      "prepared_df.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 152:\n",
      "\n",
      "prepared_df.info()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 153:\n",
      "\n",
      "prepared_df.describe()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 154:\n",
      "\n",
      "prepared_df.shape\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 155:\n",
      "\n",
      "prepared_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 156:\n",
      "\n",
      "# Drop rows with missing values in specific columns\n",
      "prepared_df = prepared_df.dropna(subset=['rain', 'temp', 'principal_component', 'dry', 'warm'])\n",
      "\n",
      "prepared_df.isnull().sum()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 157:\n",
      "\n",
      "save_chunks(prepared_df,'prepared_data')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 158:\n",
      "\n",
      "save_files(prepared_df,'prepared_data')\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 159:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 160:\n",
      "\n",
      "prepared_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 161:\n",
      "\n",
      "# Let's try grouping and plotting a small portion of the data to ensure everything works correctly\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Filter the relevant columns\n",
      "linechart_data = prepared_df[['day_type', 'cluster', 'hour', 'occupancy_pct']]\n",
      "\n",
      "# Update clusters to descriptive names\n",
      "linechart_data['cluster'] = np.where(linechart_data['cluster'] == 0, 'City Centre',\n",
      "                                     np.where(linechart_data['cluster'] == 1, 'Transition Zone',\n",
      "                                              np.where(linechart_data['cluster'] == 2, 'Transport Hubs',\n",
      "                                                       np.where(linechart_data['cluster'] == 3, 'Docklands & South City', 'Outer Suburbs'))))\n",
      "\n",
      "\n",
      "\n",
      "# Weekday data preparation\n",
      "linechart_data_weekday = linechart_data[linechart_data['day_type'] == 'weekday']\n",
      "linechart_data_weekday = linechart_data_weekday.groupby(['hour', 'cluster'])['occupancy_pct'].mean().reset_index()\n",
      "df1 = pd.DataFrame(dict(x=linechart_data_weekday['hour'].values, y=linechart_data_weekday['occupancy_pct'].values, label=linechart_data_weekday['cluster'].values))\n",
      "groups1 = df1.groupby('label')\n",
      "\n",
      "# Saturday data preparation\n",
      "linechart_data_saturday = linechart_data[linechart_data['day_type'] == 'saturday']\n",
      "linechart_data_saturday = linechart_data_saturday.groupby(['hour', 'cluster'])['occupancy_pct'].mean().reset_index()\n",
      "df2 = pd.DataFrame(dict(x=linechart_data_saturday['hour'].values, y=linechart_data_saturday['occupancy_pct'].values, label=linechart_data_saturday['cluster'].values))\n",
      "groups2 = df2.groupby('label')\n",
      "\n",
      "# Sunday data preparation\n",
      "linechart_data_sunday = linechart_data[linechart_data['day_type'] == 'sunday']\n",
      "linechart_data_sunday = linechart_data_sunday.groupby(['hour', 'cluster'])['occupancy_pct'].mean().reset_index()\n",
      "df3 = pd.DataFrame(dict(x=linechart_data_sunday['hour'].values, y=linechart_data_sunday['occupancy_pct'].values, label=linechart_data_sunday['cluster'].values))\n",
      "groups3 = df3.groupby('label')\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 162:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 163:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Weekday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups1:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Weekday Traffic Patterns per Cluster')  # Set the plot title for weekdays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Occupancy %')  # Label for the y-axis (occupancy percentage)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('weekday_traffic_patterns_per_cluster_type')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 164:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Saturday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups2:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Saturday Traffic Patterns per Cluster')  # Set the plot title for Saturdays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Occupancy %')  # Label for the y-axis (occupancy percentage)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('saturday_traffic_patterns_per_cluster_type')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 165:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Sunday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups3:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Sunday Traffic Counts per Cluster in Dublin Bikes')  # Set the plot title for Sundays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Occupancy %')  # Label for the y-axis (occupancy percentage)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box(f\"{to_snake_case(plot_title)}.png\")\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image(f\"{to_snake_case(plot_title)}.png\")\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 166:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import numpy as np\n",
      "\n",
      "# Create a new column to indicate weekday/weekend\n",
      "dublinbikes_df['day_type'] = np.where(dublinbikes_df['day_number'] <= 4, 'Weekday', 'Weekend')\n",
      "\n",
      "# Aggregate the data by hour and day type\n",
      "hourly_counts = dublinbikes_df.groupby(['hour', 'day_type'])['bike_arr'].mean().reset_index()\n",
      "\n",
      "# Plotting the data\n",
      "fig, ax = plt.subplots(figsize=(20, 10))  # Set the figure size to 20x10 inches\n",
      "sns.pointplot(data=hourly_counts, x='hour', y='bike_arr', hue='day_type', ax=ax)  # Create a point plot\n",
      "ax.set(title='Count of Bikes During Weekdays and Weekends', xlabel='Hour of Day', ylabel='Bike Activity Count')  # Set title and labels\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 167:\n",
      "\n",
      "prepared_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 168:\n",
      "\n",
      "# Let's try grouping and plotting a small portion of the trip_count data this time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Filter the relevant columns\n",
      "trip_count_linechart_data= prepared_df[['day_type', 'cluster', 'hour', 'trip_count']]\n",
      "\n",
      "# Update clusters to descriptive names\n",
      "trip_count_linechart_data['cluster'] = np.where(linechart_data['cluster'] == 0, 'City Centre',\n",
      "                                     np.where(linechart_data['cluster'] == 1, 'Transition Zone',\n",
      "                                              np.where(linechart_data['cluster'] == 2, 'Transport Hubs',\n",
      "                                                       np.where(linechart_data['cluster'] == 3, 'Docklands & South City', 'Outer Suburbs'))))\n",
      "trip_count_linechart_data.head()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 169:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Filter the relevant columns\n",
      "linechart_data = prepared_df[['day_type', 'cluster', 'hour', 'trip_count']]\n",
      "\n",
      "# Update clusters to descriptive names\n",
      "linechart_data['cluster'] = np.where(linechart_data['cluster'] == 0, 'City Centre',\n",
      "                                     np.where(linechart_data['cluster'] == 1, 'Transition Zone',\n",
      "                                              np.where(linechart_data['cluster'] == 2, 'Transport Hubs',\n",
      "                                                       np.where(linechart_data['cluster'] == 3, 'Docklands & South City', 'Outer Suburbs'))))\n",
      "\n",
      "# Weekday data preparation\n",
      "linechart_data_weekday = linechart_data[linechart_data['day_type'] == 'weekday']\n",
      "linechart_data_weekday = linechart_data_weekday.groupby(['hour', 'cluster'])['trip_count'].mean().reset_index()\n",
      "df1 = pd.DataFrame(dict(x=linechart_data_weekday['hour'].values, y=linechart_data_weekday['trip_count'].values, label=linechart_data_weekday['cluster'].values))\n",
      "groups1 = df1.groupby('label')\n",
      "\n",
      "# Saturday data preparation\n",
      "linechart_data_saturday = linechart_data[linechart_data['day_type'] == 'saturday']\n",
      "linechart_data_saturday = linechart_data_saturday.groupby(['hour', 'cluster'])['trip_count'].mean().reset_index()\n",
      "df2 = pd.DataFrame(dict(x=linechart_data_saturday['hour'].values, y=linechart_data_saturday['trip_count'].values, label=linechart_data_saturday['cluster'].values))\n",
      "groups2 = df2.groupby('label')\n",
      "\n",
      "# Sunday data preparation\n",
      "linechart_data_sunday = linechart_data[linechart_data['day_type'] == 'sunday']\n",
      "linechart_data_sunday = linechart_data_sunday.groupby(['hour', 'cluster'])['trip_count'].mean().reset_index()\n",
      "df3 = pd.DataFrame(dict(x=linechart_data_sunday['hour'].values, y=linechart_data_sunday['trip_count'].values, label=linechart_data_sunday['cluster'].values))\n",
      "groups3 = df3.groupby('label')\n",
      "\n",
      "# Plotting (example for weekday)\n",
      "plt.figure(figsize=(10, 6))\n",
      "for name, group in groups1:\n",
      "    plt.plot(group.x, group.y, marker='o', linestyle='-', label=name)\n",
      "plt.legend()\n",
      "plt.title('Average Trip Count by Hour and Cluster (Weekday)')\n",
      "plt.xlabel('Hour')\n",
      "plt.ylabel('Average Trip Count')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 170:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Weekday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups1:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Weekday Trip Counts per Cluster in Dublin Bikes')  # Set the plot title for weekdays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Average Trip Count')  # Label for the y-axis (average trip count)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('weekday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image('weekday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 171:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Saturday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups2:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Saturday Trip Counts per Cluster in Dublin Bikes')  # Set the plot title for Saturdays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Average Trip Count')  # Label for the y-axis (average trip count)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('saturday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image('saturday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 172:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot Sunday data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for name, group in groups3:\n",
      "    ax.plot(group.x, group.y, label=name)  # Plot each group with its label\n",
      "\n",
      "ax.set_title('Sunday Trip Counts per Cluster in Dublin Bikes')  # Set the plot title for Sundays\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Average Trip Count')  # Label for the y-axis (average trip count)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('sunday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image('sunday_trip_counts_per_cluster.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 173:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Aggregate data for Weekday vs Weekend comparison\n",
      "weekday_weekend_data = linechart_data.groupby(['hour', 'day_type'])['trip_count'].mean().reset_index()\n",
      "\n",
      "# Plot Weekday vs Weekend data\n",
      "fig, ax = plt.subplots(figsize=(9, 6))  # Create a figure and axis with specified size\n",
      "for day_type, group in weekday_weekend_data.groupby('day_type'):\n",
      "    ax.plot(group['hour'], group['trip_count'], marker='o', linestyle='-', label=day_type.capitalize())\n",
      "\n",
      "ax.set_title('Average Trip Counts by Hour (Weekday vs Weekend) in Dublin Bikes')  # Set the plot title\n",
      "ax.set_xlabel('Hour')  # Label for the x-axis (time of day)\n",
      "ax.set_ylabel('Average Trip Count')  # Label for the y-axis (average trip count)\n",
      "\n",
      "# Move the legend to the right side of the graph\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small', borderaxespad=0)\n",
      "\n",
      "# Save the figure using the save_box function\n",
      "save_box('weekday_vs_weekend_trip_counts.png')\n",
      "\n",
      "# Save the image using the custom save_image function\n",
      "save_image('weekday_vs_weekend_trip_counts.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 174:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 175:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 176:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Assuming 'prepared_df' has similar structure to 'merged_with_clusters'\n",
      "\n",
      "# Group by station_id, station_name, and date to calculate total rain and bike changes (arrivals/departures)\n",
      "impact_df = prepared_df.groupby(['station_id', 'station_name', 'date']).agg(\n",
      "    rain=('rain', 'sum'), \n",
      "    TOTAL_CHANGES=('trip_count', 'sum')\n",
      ").reset_index()\n",
      "\n",
      "# Define wet/dry days based on accumulated daily rain\n",
      "impact_df['WET/DRY DAY'] = np.where(impact_df['rain'] > 3, \"Wet\", \"Dry\")\n",
      "\n",
      "# Drop the 'rain' column as it's no longer needed\n",
      "impact_df = impact_df.drop(['rain'], axis=1).reset_index(drop=True)\n",
      "\n",
      "# Merge the wet/dry day information back into the original dataset\n",
      "merged_with_wetdry = pd.merge(\n",
      "    prepared_df, \n",
      "    impact_df[['station_id', 'station_name', 'date', 'WET/DRY DAY', 'TOTAL_CHANGES']], \n",
      "    on=['station_id', 'station_name', 'date'], \n",
      "    how='left'\n",
      ")\n",
      "\n",
      "# Group by station_id, station_name, and wet/dry day to calculate average changes\n",
      "wetday_avg_changes = merged_with_wetdry.groupby(['station_id', 'station_name', 'WET/DRY DAY']).agg(\n",
      "    AVG_CHANGES=('TOTAL_CHANGES', 'mean')\n",
      ").reset_index()\n",
      "\n",
      "# Calculate the difference in average changes between dry and wet days\n",
      "difference_df = wetday_avg_changes.pivot(index='station_name', columns='WET/DRY DAY', values='AVG_CHANGES').reset_index()\n",
      "\n",
      "# Calculate the change in average bike activity between dry and wet days\n",
      "difference_df['Change'] = difference_df['Dry'] - difference_df['Wet']\n",
      "\n",
      "# Sort the results by the difference and display the top 30 stations\n",
      "top_30_stations = difference_df.sort_values(by='Change', ascending=False).head(30)\n",
      "\n",
      "# Display the top 30 stations with the largest difference\n",
      "top_30_stations\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 177:\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Ensure the correct column names are used\n",
      "y = wetday_df['AVG_CHANGES'].values  # Use 'AVG_CHANGES' as the y-axis values\n",
      "x = wetday_df['station_id'].values   # Use 'station_id' as the x-axis values\n",
      "labels = wetday_df['WET/DRY DAY'].values  # Use 'WET/DRY DAY' as labels for grouping\n",
      "\n",
      "# Create a DataFrame for grouping\n",
      "df = pd.DataFrame(dict(x=x, y=y, label=labels))\n",
      "\n",
      "# Group by 'WET/DRY DAY'\n",
      "groups = df.groupby('label')\n",
      "\n",
      "# Plot settings\n",
      "SMALL_SIZE = 20\n",
      "MEDIUM_SIZE = 20\n",
      "BIGGER_SIZE = 22\n",
      "\n",
      "plt.rc('font', size=SMALL_SIZE)\n",
      "plt.rc('axes', titlesize=SMALL_SIZE)\n",
      "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
      "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
      "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
      "plt.rc('legend', fontsize=SMALL_SIZE)\n",
      "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
      "\n",
      "# Plotting the data\n",
      "fig, ax = plt.subplots(figsize=(14, 8))  # Set the figure size\n",
      "\n",
      "for name, group in groups:\n",
      "    ax.plot(group.x, group.y, marker='o', linestyle='-', label=name)  # Plot each group\n",
      "\n",
      "# Adding title and labels\n",
      "ax.set_title('Average Average Rentals Per Day by Station ID on Wet and Dry Days)')\n",
      "ax.set_xlabel('Station ID')\n",
      "ax.set_ylabel('Average Changes')\n",
      "\n",
      "# Adding legend\n",
      "ax.legend(title='Wet/Dry Day', loc='best')\n",
      "\n",
      "\n",
      "plt.savefig('mlimages/wetdryaverage_changes_by_station_id.png', dpi=300, bbox_inches='tight')\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 178:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 179:\n",
      "\n",
      "prepared_df.columns\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 180:\n",
      "\n",
      "import warnings\n",
      "import pandas as pd\n",
      "from statsmodels.tsa.arima.model import ARIMA\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "# Suppress specific warnings\n",
      "warnings.filterwarnings(\"ignore\", message=\"No frequency information was provided\")\n",
      "\n",
      "# Assuming prepared_df contains a 'date' column and 'trip_count' column\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "time_series_data = prepared_df.groupby('date')['trip_count'].sum().reset_index()\n",
      "\n",
      "# Set the date column as the index for time series analysis\n",
      "time_series_data.set_index('date', inplace=True)\n",
      "\n",
      "# Split the data into training and test sets\n",
      "train = time_series_data.loc[:'2022-06-30']  # First 1.5 years\n",
      "test = time_series_data.loc['2022-07-01':]  # Last 0.5 years\n",
      "\n",
      "# Fit an ARIMA model\n",
      "arima_model = ARIMA(train, order=(5, 1, 0))\n",
      "arima_result = arima_model.fit()\n",
      "\n",
      "# Make predictions\n",
      "predictions = arima_result.forecast(steps=len(test))\n",
      "mse = mean_squared_error(test, predictions)\n",
      "\n",
      "print(f'Mean Squared Error: {mse}')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 181:\n",
      "\n",
      "\n",
      "prepared_df.nunique()\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 182:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 183:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure the date column is in datetime format\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "\n",
      "# Group by date to get the total trip count for each day across all clusters\n",
      "total_traffic = prepared_df.groupby('date')['trip_count'].sum().reset_index()\n",
      "\n",
      "# Apply a 21-day rolling average to smooth the data\n",
      "total_traffic['smoothed_trip_count'] = total_traffic['trip_count'].rolling(window=21).mean()\n",
      "\n",
      "# Plotting the data\n",
      "plt.figure(figsize=(14, 8))\n",
      "plt.plot(total_traffic['date'], total_traffic['smoothed_trip_count'], label='Total Traffic', color='blue')\n",
      "\n",
      "plt.title('Smoothed Daily Total Traffic Across the Network Over Two Years (21-Day Rolling Average)')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Total Trip Count (21-Day Rolling Average)')\n",
      "plt.legend(title='Traffic')\n",
      "plt.grid(True)\n",
      "\n",
      "# Save the figure using the save_index_image function\n",
      "save_index_image('Smoothed_Daily_Total_Traffic_Over_Two_Years.png')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 184:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure the date column is in datetime format\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "\n",
      "# Group by date and cluster to get the total trip count for each cluster per day\n",
      "cluster_traffic = prepared_df.groupby(['date', 'cluster'])['trip_count'].sum().reset_index()\n",
      "\n",
      "# Pivot the data to have dates as the index and clusters as columns\n",
      "cluster_traffic_pivot = cluster_traffic.pivot(index='date', columns='cluster', values='trip_count')\n",
      "\n",
      "# Apply a 21-day rolling average to smooth the data\n",
      "cluster_traffic_smoothed = cluster_traffic_pivot.rolling(window=21).mean()\n",
      "\n",
      "# Plotting the data\n",
      "plt.figure(figsize=(14, 8))\n",
      "for cluster in cluster_traffic_smoothed.columns:\n",
      "    plt.plot(cluster_traffic_smoothed.index, cluster_traffic_smoothed[cluster], label=f'Cluster {cluster}')\n",
      "\n",
      "plt.title('Smoothed Daily Traffic Across Clusters Over Two Years (21-Day Rolling Average)')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Total Trip Count (21-Day Rolling Average)')\n",
      "plt.legend(title='Cluster')\n",
      "plt.grid(True)\n",
      "save_index_image('Smoothed Daily Traffic Across Clusters Over Two Years')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 185:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure the date column is in datetime format\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "\n",
      "# Define weekdays and weekends\n",
      "prepared_df['day_type'] = prepared_df['date'].dt.dayofweek.apply(lambda x: 'Weekday' if x < 5 else 'Weekend')\n",
      "\n",
      "# Group by date and day_type to get the total trip count for each day across all clusters\n",
      "total_traffic_daytype = prepared_df.groupby(['date', 'day_type'])['trip_count'].sum().reset_index()\n",
      "\n",
      "# Separate data for weekdays and weekends\n",
      "weekday_traffic = total_traffic_daytype[total_traffic_daytype['day_type'] == 'Weekday']\n",
      "weekend_traffic = total_traffic_daytype[total_traffic_daytype['day_type'] == 'Weekend']\n",
      "\n",
      "# Calculate total daily traffic across all days (not separating weekdays and weekends)\n",
      "total_traffic = prepared_df.groupby('date')['trip_count'].sum().reset_index()\n",
      "\n",
      "# Apply a 21-day rolling average to smooth the data\n",
      "weekday_traffic['smoothed_trip_count'] = weekday_traffic['trip_count'].rolling(window=21).mean()\n",
      "weekend_traffic['smoothed_trip_count'] = weekend_traffic['trip_count'].rolling(window=21).mean()\n",
      "\n",
      "\n",
      "# Plotting the data for weekdays, weekends, and totals on the same graph\n",
      "plt.figure(figsize=(14, 8))\n",
      "plt.plot(weekday_traffic['date'], weekday_traffic['smoothed_trip_count'], label='Weekday Traffic', color='blue')\n",
      "plt.plot(weekend_traffic['date'], weekend_traffic['smoothed_trip_count'], label='Weekend Traffic', color='red')\n",
      "\n",
      "\n",
      "plt.title('Smoothed Daily Total Traffic Over Two Years (21-Day Rolling Average)')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Total Trip Count (21-Day Rolling Average)')\n",
      "plt.legend(title='Traffic Type')\n",
      "plt.grid(True)\n",
      "\n",
      "# Save the figure using the save_index_image function\n",
      "save_index_image('Smoothed_Daily_Weekday_Weekend_Total_Traffic_Over_Two_Years.png')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 186:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# Filter the DataFrame to include only cluster 1\n",
      "cluster_1_data = prepared_df[prepared_df['cluster'] == 1]\n",
      "\n",
      "# Group by station to calculate the average trip count per station\n",
      "average_trip_count_per_station = cluster_1_data.groupby('station_name')['trip_count'].mean().reset_index()\n",
      "\n",
      "# Sort the stations by average trip count for better visualization\n",
      "average_trip_count_per_station = average_trip_count_per_station.sort_values(by='trip_count', ascending=False)\n",
      "\n",
      "# Plotting the data\n",
      "plt.figure(figsize=(14, 12))\n",
      "plt.barh(average_trip_count_per_station['station_name'], average_trip_count_per_station['trip_count'], color='teal')\n",
      "\n",
      "plt.title('Average Trip Count per Station in Cluster 1')\n",
      "plt.xlabel('Average Trip Count')\n",
      "plt.ylabel('Station Name')\n",
      "plt.grid(True, axis='x')\n",
      "\n",
      "# Save the figure using the save_index_image function\n",
      "save_index_image('Average_Trip_Count_per_Station_in_Cluster_1.png')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 187:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# Filter the DataFrame to include only cluster 1\n",
      "cluster_1_data = prepared_df[prepared_df['cluster'] == 2]\n",
      "\n",
      "# Group by station to calculate the average trip count per station\n",
      "average_trip_count_per_station = cluster_1_data.groupby('station_name')['trip_count'].mean().reset_index()\n",
      "\n",
      "# Sort the stations by average trip count for better visualization\n",
      "average_trip_count_per_station = average_trip_count_per_station.sort_values(by='trip_count', ascending=False)\n",
      "\n",
      "# Plotting the data\n",
      "plt.figure(figsize=(14, 8))\n",
      "plt.barh(average_trip_count_per_station['station_name'], average_trip_count_per_station['trip_count'], color='teal')\n",
      "\n",
      "plt.title('Average Trip Count per Station in Cluster 1')\n",
      "plt.xlabel('Average Trip Count')\n",
      "plt.ylabel('Station Name')\n",
      "plt.grid(True, axis='x')\n",
      "\n",
      "# Save the figure using the save_index_image function\n",
      "save_index_image('Average_Trip_Count_per_Station_in_Cluster_1.png')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 188:\n",
      "\n",
      "prepared_df['cluster'].unique()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 189:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 190:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "def plot_average_trip_count_per_station(prepared_df, cluster_number):\n",
      "    # Filter the DataFrame to include only the specified cluster\n",
      "    cluster_data = prepared_df[prepared_df['cluster'] == cluster_number]\n",
      "\n",
      "    # Group by station to calculate the average trip count per station\n",
      "    average_trip_count_per_station = cluster_data.groupby('station_name')['trip_count'].mean().reset_index()\n",
      "\n",
      "    # Sort the stations by average trip count for better visualization\n",
      "    average_trip_count_per_station = average_trip_count_per_station.sort_values(by='trip_count', ascending=False)\n",
      "\n",
      "    # Plotting the data with the specified figure size\n",
      "    plt.figure(figsize=(14, 12))\n",
      "    plt.barh(average_trip_count_per_station['station_name'], average_trip_count_per_station['trip_count'], color='teal')\n",
      "\n",
      "    # Set the title and labels dynamically based on the cluster number\n",
      "    plt.title(f'Average Trip Count per Station in Cluster {cluster_number}')\n",
      "    plt.xlabel('Average Trip Count')\n",
      "    plt.ylabel('Station Name')\n",
      "    plt.grid(True, axis='x')\n",
      "\n",
      "    # Save the figure using the save_index_image function with a dynamic filename\n",
      "    save_index_image(f'Average_Trip_Count_per_Station_in_Cluster_{cluster_number}.png')\n",
      "\n",
      "    # Show the plot\n",
      "    plt.show()\n",
      "\n",
      "# Example usage for clusters 1 through 5\n",
      "for cluster in range(1, 6):\n",
      "    plot_average_trip_count_per_station(prepared_df, cluster)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 191:\n",
      "\n",
      "import pandas as pd\n",
      "import dash\n",
      "from dash import dcc, html\n",
      "from dash.dependencies import Input, Output\n",
      "import plotly.express as px\n",
      "\n",
      "# Prepare the data: Convert the 'date' column to datetime\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "\n",
      "# Group by station and week to calculate the average weekly trip count per station\n",
      "prepared_df['week'] = prepared_df['date'].dt.strftime('%Y-%U')  # Format week as \"Year-WeekNumber\"\n",
      "average_weekly_traffic = prepared_df.groupby(['station_name', 'week'])['trip_count'].mean().reset_index()\n",
      "\n",
      "# Initialize the Dash app\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "# Define the layout of the dashboard\n",
      "app.layout = html.Div([\n",
      "    html.H1(\"Average Weekly Traffic Across 2022 and 2023 for All Stations\"),\n",
      "    dcc.Dropdown(\n",
      "        id='station-dropdown',\n",
      "        options=[{'label': station, 'value': station} for station in average_weekly_traffic['station_name'].unique()],\n",
      "        value=average_weekly_traffic['station_name'].unique()[0],  # Default to the first station\n",
      "        clearable=False,\n",
      "        style={'width': '50%'}\n",
      "    ),\n",
      "    dcc.Graph(id='station-graph')\n",
      "])\n",
      "\n",
      "# Define the callback to update the graph based on the selected station\n",
      "@app.callback(\n",
      "    Output('station-graph', 'figure'),\n",
      "    [Input('station-dropdown', 'value')]\n",
      ")\n",
      "def update_graph(selected_station):\n",
      "    # Filter the data for the selected station\n",
      "    filtered_data = average_weekly_traffic[average_weekly_traffic['station_name'] == selected_station]\n",
      "    \n",
      "    # Create the plot using Plotly Express\n",
      "    fig = px.line(filtered_data, x='week', y='trip_count',\n",
      "                  labels={'trip_count': 'Average Trip Count', 'week': 'Week'},\n",
      "                  title=f'Average Weekly Traffic for {selected_station} Across the Year',\n",
      "                  color_discrete_sequence=['teal'])\n",
      "    \n",
      "    # Update x-axis to display weeks properly\n",
      "    fig.update_xaxes(type='category', categoryorder='category ascending')\n",
      "\n",
      "    return fig\n",
      "\n",
      "# Run the app\n",
      "if __name__ == '__main__':\n",
      "    app.run_server(debug=True)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 192:\n",
      "\n",
      "import pandas as pd\n",
      "import dash\n",
      "from dash import dcc, html\n",
      "from dash.dependencies import Input, Output\n",
      "import plotly.express as px\n",
      "\n",
      "# Prepare the data: Convert the 'date' column to datetime\n",
      "prepared_df['date'] = pd.to_datetime(prepared_df['date'])\n",
      "\n",
      "# Extract the day of the week from the date (0=Monday, 6=Sunday)\n",
      "prepared_df['day_of_week'] = prepared_df['date'].dt.dayofweek\n",
      "\n",
      "# Group by station and day_of_week to calculate the average daily trip count per station\n",
      "average_daily_traffic = prepared_df.groupby(['station_name', 'day_of_week'])['trip_count'].mean().reset_index()\n",
      "\n",
      "# Map day_of_week to actual day names for better readability\n",
      "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
      "average_daily_traffic['day_of_week'] = average_daily_traffic['day_of_week'].map(day_mapping)\n",
      "\n",
      "# Initialize the Dash app\n",
      "app = dash.Dash(__name__)\n",
      "\n",
      "# Define the layout of the dashboard\n",
      "app.layout = html.Div([\n",
      "    html.H1(\"Average Daily Traffic Across the Week for Each Station\"),\n",
      "    dcc.Dropdown(\n",
      "        id='station-dropdown',\n",
      "        options=[{'label': station, 'value': station} for station in average_daily_traffic['station_name'].unique()],\n",
      "        value=average_daily_traffic['station_name'].unique()[0],  # Default to the first station\n",
      "        clearable=False,\n",
      "        style={'width': '50%'}\n",
      "    ),\n",
      "    dcc.Graph(id='station-graph')\n",
      "])\n",
      "\n",
      "# Define the callback to update the graph based on the selected station\n",
      "@app.callback(\n",
      "    Output('station-graph', 'figure'),\n",
      "    [Input('station-dropdown', 'value')]\n",
      ")\n",
      "def update_graph(selected_station):\n",
      "    # Filter the data for the selected station\n",
      "    filtered_data = average_daily_traffic[average_daily_traffic['station_name'] == selected_station]\n",
      "    \n",
      "    # Create the plot using Plotly Express\n",
      "    fig = px.line(filtered_data, x='day_of_week', y='trip_count',\n",
      "                  labels={'trip_count': 'Average Trip Count', 'day_of_week': 'Day of the Week'},\n",
      "                  title=f'Average Daily Traffic for {selected_station} Across the Week',\n",
      "                  markers=True)\n",
      "    \n",
      "    # Update x-axis to reflect the correct order of days\n",
      "    fig.update_xaxes(categoryorder='array', categoryarray=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
      "\n",
      "    return fig\n",
      "\n",
      "# Run the app\n",
      "if __name__ == '__main__':\n",
      "    app.run_server(debug=True)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 193:\n",
      "\n",
      "notebook_end_time = time.time()\n",
      "elapsed_time = notebook_end_time - notebook_start_time\n",
      "print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
      "\n",
      "================================================================================\n",
      "\n",
      "All code cells have been copied to the clipboard.\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import pyperclip\n",
    "\n",
    "# Path to your Jupyter Notebook file\n",
    "notebook_path = '200_preparation_final.ipynb'\n",
    "\n",
    "# Read the notebook file\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Extract all code cells\n",
    "code_cells = [cell['source'] for cell in notebook.cells if cell.cell_type == 'code']\n",
    "\n",
    "# Join all code cells into a single string\n",
    "code_cells_text = '\\n\\n'.join(code_cells)\n",
    "\n",
    "# Copy to clipboard\n",
    "pyperclip.copy(code_cells_text)\n",
    "\n",
    "# Display the extracted code cells (optional)\n",
    "for i, code in enumerate(code_cells):\n",
    "    print(f'Code Cell {i+1}:\\n')\n",
    "    print(code)\n",
    "    print('\\n' + '='*80 + '\\n')\n",
    "\n",
    "print(\"All code cells have been copied to the clipboard.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735ce3e7-e6a7-477f-a7a3-dae4a2375976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Cell 1:\n",
      "\n",
      "# bbbpip install praw\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 2:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "# Example target URL (replace with actual Washington-based news URL)\n",
      "url = 'https://www.seattletimes.com/seattle-news/transportation/'\n",
      "\n",
      "# Send a request to fetch the page content\n",
      "response = requests.get(url)\n",
      "response.raise_for_status()  # Ensure we got a successful response\n",
      "\n",
      "# Parse the HTML content\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "\n",
      "# Find all articles or relevant content sections (adjust tags/classes as needed)\n",
      "articles = soup.find_all('div', class_='article__details')\n",
      "\n",
      "# Prepare lists to store extracted data\n",
      "titles = []\n",
      "summaries = []\n",
      "links = []\n",
      "\n",
      "# Loop through the found articles and extract the title, summary, and link\n",
      "for article in articles:\n",
      "    title_tag = article.find('h3')\n",
      "    summary_tag = article.find('p')\n",
      "    link_tag = article.find('a')\n",
      "\n",
      "    if title_tag and summary_tag and link_tag:\n",
      "        title = title_tag.text.strip()\n",
      "        summary = summary_tag.text.strip()\n",
      "        link = link_tag['href']\n",
      "\n",
      "        titles.append(title)\n",
      "        summaries.append(summary)\n",
      "        links.append(link)\n",
      "\n",
      "# Create a DataFrame to store the scraped data\n",
      "df = pd.DataFrame({\n",
      "    'Title': titles,\n",
      "    'Summary': summaries,\n",
      "    'Link': links\n",
      "})\n",
      "\n",
      "# Print the first few rows to verify the results\n",
      "print(df.head())\n",
      "\n",
      "# Optionally, save the data to a CSV file\n",
      "df.to_csv('seattle_transportation_articles.csv', index=False)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 3:\n",
      "\n",
      "user_agent = 'DublinBikesSentimentAnalysis by /u/your_reddit_ronandownes'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 4:\n",
      "\n",
      "urls = {\n",
      "    # Bike Share Programs\n",
      "    'capital_bikeshare': 'https://www.capitalbikeshare.com/blog',  # Washington, D.C.\n",
      "    'citibike_nyc': 'https://citibikenyc.com/blog',  # New York City\n",
      "    'divvy_chicago': 'https://divvybikes.com/blog',  # Chicago\n",
      "    'bikeshare_london': 'https://tfl.gov.uk/modes/cycling/santander-cycles',  # London\n",
      "    'bikeshare_dublin': 'https://www.bikeshare.ie/',  # Dublin\n",
      "    'nextbike_berlin': 'https://www.nextbike.de/en/berlin/news/',  # Berlin\n",
      "    'bikemi_milan': 'https://www.bikemi.com/',  # Milan\n",
      "    'bicing_barcelona': 'https://www.bicing.barcelona/',  # Barcelona\n",
      "    'dublin_bikes': 'https://dublinbikes.ie/News',  # Dublin, Ireland\n",
      "    \n",
      "    # Cycling Advocacy and Urban Mobility\n",
      "    'people_for_bikes': 'https://peopleforbikes.org/blog',  # US-based cycling advocacy\n",
      "    'ecf_cycling': 'https://ecf.com/news-and-events/news',  # European Cyclists' Federation\n",
      "    'cycling_uk': 'https://www.cyclinguk.org/news',  # Cycling UK, advocacy and news\n",
      "    'streetsblog_usa': 'https://usa.streetsblog.org/',  # US urban mobility and transportation blog\n",
      "    'streetsblog_nyc': 'https://nyc.streetsblog.org/',  # NYC-focused urban transportation\n",
      "    'bikeportland': 'https://bikeportland.org/',  # Portland, OR, cycling news and advocacy\n",
      "    'irish_cycle_news': 'https://irishcycle.com/',  # Irish cycling news and advocacy\n",
      "    \n",
      "    # Cycling Forums and Communities\n",
      "    'reddit_cycling': 'https://www.reddit.com/r/cycling/',  # Reddit Cycling community\n",
      "    'reddit_bikeshare': 'https://www.reddit.com/r/bikeshare/',  # Reddit BikeShare community\n",
      "    'the_urbanist': 'https://www.theurbanist.org/',  # Urban planning and transportation blog\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 5:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "# Loop through each URL in the dictionary\n",
      "for name, url in urls.items():\n",
      "    print(f\"Scraping {name} from {url}\")\n",
      "    \n",
      "    response = requests.get(url)\n",
      "    response.raise_for_status()\n",
      "\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    \n",
      "    # Adjust the scraping logic based on the site's HTML structure\n",
      "    if name == 'bikeshare_home':\n",
      "        # Example: Scrape announcements or news on the homepage\n",
      "        articles = soup.find_all('div', class_='announcement')\n",
      "        titles, details, links = [], [], []\n",
      "        for article in articles:\n",
      "            title_tag = article.find('h3')\n",
      "            detail_tag = article.find('p')\n",
      "            link_tag = article.find('a')\n",
      "            if title_tag and detail_tag and link_tag:\n",
      "                title = title_tag.text.strip()\n",
      "                detail = detail_tag.text.strip()\n",
      "                link = link_tag['href']\n",
      "                titles.append(title)\n",
      "                details.append(detail)\n",
      "                links.append(link)\n",
      "        df = pd.DataFrame({'Title': titles, 'Detail': details, 'Link': links})\n",
      "        df.to_csv(f'{name}.csv', index=False)\n",
      "    \n",
      "    # Add similar blocks for other sites as needed\n",
      "\n",
      "print(\"Scraping completed.\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 6:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 7:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 8:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 9:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 10:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 11:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 12:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 13:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 14:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 15:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Load the HTML content from the file\n",
      "with open('/mnt/data/Walking and Cycling Index 2023 - Publication reports - National Transport.html', 'r', encoding='utf-8') as file:\n",
      "    page_content = file.read()\n",
      "\n",
      "# Parse the HTML content with BeautifulSoup\n",
      "soup = BeautifulSoup(page_content, 'html.parser')\n",
      "\n",
      "# Find all sections that contain publication data\n",
      "publications = soup.find_all('div', class_='file-download')\n",
      "\n",
      "# Extract and print titles and links to the PDF files\n",
      "for publication in publications:\n",
      "    title = publication.find('h3').get_text(strip=True)\n",
      "    link = publication.find('a')['href']\n",
      "    print(f'Title: {title}')\n",
      "    print(f'Link: {link}\\n')\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 16:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 17:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 18:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 19:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 20:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 21:\n",
      "\n",
      "import praw\n",
      "\n",
      "# Set up your Reddit API credentials\n",
      "reddit = praw.Reddit(client_id='Ra25DELSPPefF_NvxjSAmQ', \n",
      "                     client_secret='J89NoatiJdC3aMf0g1k3rJ02-SvpSA', \n",
      "                     user_agent=user_agent)  # Replace with your Reddit username\n",
      "\n",
      "# Choose a subreddit related to cycling or biking (you can use a different one if needed)\n",
      "subreddit = reddit.subreddit('dublinbikes')\n",
      "\n",
      "# Collect posts related to bike sharing\n",
      "posts = []\n",
      "for submission in subreddit.search('bike share OR DublinBikes OR bike empty OR bike full', limit=100):\n",
      "    posts.append(submission.title + \" \" + submission.selftext)\n",
      "\n",
      "# Print the number of posts collected\n",
      "print(f\"Collected {len(posts)} posts\")\n",
      "\n",
      "# Optional: Print the posts to verify\n",
      "for post in posts:\n",
      "    print(post)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 22:\n",
      "\n",
      "for submission in subreddit.search('bike', limit=100):\n",
      "    posts.append(submission.title + \" \" + submission.selftext)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 23:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 24:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 25:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "from textblob import TextBlob\n",
      "\n",
      "# The URL of the website's page you want to scrape\n",
      "url = 'https://irishcycle.com/'\n",
      "\n",
      "# Make a request to the website\n",
      "response = requests.get(url)\n",
      "response.raise_for_status()  # Ensure we got a successful response\n",
      "\n",
      "# Parse the HTML content\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "\n",
      "# Find the articles on the page\n",
      "articles = soup.find_all('article')\n",
      "\n",
      "# Prepare lists to store the extracted data\n",
      "titles = []\n",
      "contents = []\n",
      "\n",
      "# Loop through the found articles and extract the title and content\n",
      "for article in articles:\n",
      "    # You might need to adjust these selectors based on your inspection\n",
      "    title_tag = article.find('h2', class_='entry-title')  # Update this line if needed\n",
      "    content_tag = article.find('div', class_='entry-content')  # Update this line if needed\n",
      "    \n",
      "    if title_tag and content_tag:  # Check if both tags were found\n",
      "        title = title_tag.text.strip()\n",
      "        content = content_tag.text.strip()\n",
      "        titles.append(title)\n",
      "        contents.append(content)\n",
      "\n",
      "# Create a DataFrame to store the scraped data\n",
      "df = pd.DataFrame({\n",
      "    'Title': titles,\n",
      "    'Content': contents\n",
      "})\n",
      "\n",
      "# Perform sentiment analysis on the content\n",
      "df['Sentiment'] = df['Content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
      "\n",
      "# Print the first few rows to verify the results\n",
      "print(df.head())\n",
      "\n",
      "# Optionally, save the data to a CSV file\n",
      "df.to_csv('irishcycle_articles.csv', index=False)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 26:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 27:\n",
      "\n",
      "user_agent='BikeShareSentimentAnalysis by /u/ronan_downes'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 28:\n",
      "\n",
      "import praw\n",
      "\n",
      "# Set up your Reddit API credentials\n",
      "reddit = praw.Reddit(client_id='J89NoatiJdC3aMf0g1k3rJ02-SvpSA', \n",
      "                     client_secret='Ra25DELSPPefF_NvxjSAmQ', \n",
      "                     user_agent='BikeShareSentimentAnalysis by /u/ronan_downes')\n",
      "\n",
      "# Choose a subreddit related to cycling\n",
      "subreddit = reddit.subreddit('bicycling')\n",
      "\n",
      "# Collect posts related to bike sharing\n",
      "posts = []\n",
      "for submission in subreddit.search('bike share', limit=100):  # Adjust the query as needed\n",
      "    posts.append(submission.title + \" \" + submission.selftext)\n",
      "\n",
      "# Print the number of posts collected\n",
      "print(f\"Collected {len(posts)} posts\")\n",
      "\n",
      "# Optional: Print the posts to verify\n",
      "for post in posts:\n",
      "    print(post)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 29:\n",
      "\n",
      "# pip install requests beautifulsoup4 pandas\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 30:\n",
      "\n",
      "import tweepy\n",
      "\n",
      "# Set up your Twitter API credentials\n",
      "api_key = 'QxgAp3BL2fUQqwr4ubWJxMlnA'\n",
      "api_secret = 'UJ2ozEwODk7RFEVf76lvGKHjtxGwcByrtLT7Hw812eKlJDZgCG'\n",
      "access_token = '18135960-qjJP4CQvXZrZufAVzE8FOSiKKM7JYD5wy8Jaozv4z'\n",
      "access_token_secret = '84KwV0wLGPN38dtjn90Ontxy1bvVOoPSJjBctuXs0Sr92'\n",
      "\n",
      "# Authenticate to Twitter\n",
      "auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "# Define a query to search for tweets about cycling\n",
      "query = \"cycling OR bike OR bicycle -filter:retweets\"  # Exclude retweets\n",
      "\n",
      "# Collect tweets\n",
      "tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode='extended').items(100)\n",
      "\n",
      "# Extract the text from tweets\n",
      "tweet_texts = [tweet.full_text for tweet in tweets]\n",
      "\n",
      "# Print the number of tweets collected\n",
      "print(f\"Collected {len(tweet_texts)} tweets\")\n",
      "\n",
      "# Optional: Print the tweets to verify\n",
      "for tweet in tweet_texts:\n",
      "    print(tweet)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 31:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 32:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 33:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 34:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 35:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 36:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 37:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 38:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 39:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "# Define a function to scrape a single URL\n",
      "def scrape_website(url):\n",
      "    response = requests.get(url)\n",
      "    if response.status_code == 200:\n",
      "        soup = BeautifulSoup(response.content, 'html.parser')\n",
      "        return soup\n",
      "    else:\n",
      "        print(f\"Failed to retrieve {url}\")\n",
      "        return None\n",
      "\n",
      "# Example usage: Scraping the homepage of irishcycle.com\n",
      "url = 'https://irishcycle.com/'\n",
      "soup = scrape_website(url)\n",
      "\n",
      "if soup:\n",
      "    # Example: Find all article titles\n",
      "    titles = soup.find_all('h2', class_='entry-title')\n",
      "    for title in titles:\n",
      "        print(title.get_text())\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 40:\n",
      "\n",
      "# Function to scrape articles from irishcycle.com\n",
      "def scrape_irishcycle():\n",
      "    url = 'https://irishcycle.com/'\n",
      "    soup = scrape_website(url)\n",
      "    \n",
      "    if soup:\n",
      "        # Find all article titles and links\n",
      "        articles = []\n",
      "        titles = soup.find_all('h2', class_='entry-title')\n",
      "        for title in titles:\n",
      "            article_title = title.get_text()\n",
      "            article_link = title.find('a')['href']\n",
      "            articles.append({'title': article_title, 'link': article_link})\n",
      "        \n",
      "        # Convert the list of articles to a DataFrame\n",
      "        df_articles = pd.DataFrame(articles)\n",
      "        print(df_articles)\n",
      "        \n",
      "        # Save the data to a CSV file\n",
      "        df_articles.to_csv('irishcycle_articles.csv', index=False)\n",
      "        print(\"Articles saved to irishcycle_articles.csv\")\n",
      "\n",
      "# Call the function to scrape and save data\n",
      "scrape_irishcycle()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 41:\n",
      "\n",
      "# Function to scrape articles from irishcycle.com\n",
      "def scrape_irishcycle():\n",
      "    url = 'https://irishcycle.com/'\n",
      "    soup = scrape_website(url)\n",
      "    \n",
      "    if soup:\n",
      "        # Find all article titles and links\n",
      "        articles = []\n",
      "        titles = soup.find_all('h2', class_='entry-title')\n",
      "        for title in titles:\n",
      "            article_title = title.get_text()\n",
      "            article_link = title.find('a')['href']\n",
      "            articles.append({'title': article_title, 'link': article_link})\n",
      "        \n",
      "        # Convert the list of articles to a DataFrame\n",
      "        df_articles = pd.DataFrame(articles)\n",
      "        print(df_articles)\n",
      "        \n",
      "        # Save the data to a CSV file\n",
      "        df_articles.to_csv('irishcycle_articles.csv', index=False)\n",
      "        print(\"Articles saved to irishcycle_articles.csv\")\n",
      "\n",
      "# Call the function to scrape and save data\n",
      "scrape_irishcycle()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 42:\n",
      "\n",
      "# Adjust the selector based on the HTML structure you find\n",
      "def scrape_irishcycle():\n",
      "    url = 'https://irishcycle.com/'\n",
      "    soup = scrape_website(url)\n",
      "    \n",
      "    if soup:\n",
      "        # Check the HTML structure in your browser and update selectors if necessary\n",
      "        articles = []\n",
      "        titles = soup.find_all('h2', class_='entry-title')\n",
      "        for title in titles:\n",
      "            article_title = title.get_text()\n",
      "            article_link = title.find('a')['href']\n",
      "            articles.append({'title': article_title, 'link': article_link})\n",
      "        \n",
      "        # Check if articles were found\n",
      "        if articles:\n",
      "            df_articles = pd.DataFrame(articles)\n",
      "            print(df_articles)\n",
      "            df_articles.to_csv('irishcycle_articles.csv', index=False)\n",
      "            print(\"Articles saved to irishcycle_articles.csv\")\n",
      "        else:\n",
      "            print(\"No articles found.\")\n",
      "    else:\n",
      "        print(\"Failed to retrieve content from the website.\")\n",
      "\n",
      "scrape_irishcycle()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 43:\n",
      "\n",
      "from selenium import webdriver\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "\n",
      "def scrape_irishcycle_selenium():\n",
      "    url = 'https://irishcycle.com/'\n",
      "    \n",
      "    # Set up Selenium WebDriver (e.g., for Chrome)\n",
      "    driver = webdriver.Chrome()\n",
      "    driver.get(url)\n",
      "    \n",
      "    # Get the page source after JavaScript has executed\n",
      "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
      "    driver.quit()\n",
      "    \n",
      "    articles = []\n",
      "    titles = soup.find_all('h2', class_='entry-title')\n",
      "    for title in titles:\n",
      "        article_title = title.get_text()\n",
      "        article_link = title.find('a')['href']\n",
      "        articles.append({'title': article_title, 'link': article_link})\n",
      "    \n",
      "    if articles:\n",
      "        df_articles = pd.DataFrame(articles)\n",
      "        print(df_articles)\n",
      "        df_articles.to_csv('irishcycle_articles_selenium.csv', index=False)\n",
      "        print(\"Articles saved to irishcycle_articles_selenium.csv\")\n",
      "    else:\n",
      "        print(\"No articles found.\")\n",
      "\n",
      "scrape_irishcycle_selenium()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 44:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 45:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 46:\n",
      "\n",
      "def scrape_datagov_for_dublin_bikes():\n",
      "    url = 'https://data.gov.ie/dataset?tags=cycling'\n",
      "    soup = scrape_website(url)\n",
      "    \n",
      "    if soup:\n",
      "        datasets = []\n",
      "        # Find dataset titles and descriptions\n",
      "        items = soup.find_all('div', class_='dataset-content')\n",
      "        for item in items:\n",
      "            dataset_title_tag = item.find('h3')\n",
      "            dataset_link_tag = item.find('a')\n",
      "            dataset_description_tag = item.find('div', class_='notes')\n",
      "            \n",
      "            # Check if tags exist to avoid AttributeError\n",
      "            if dataset_title_tag and dataset_link_tag and dataset_description_tag:\n",
      "                dataset_title = dataset_title_tag.get_text()\n",
      "                dataset_link = dataset_link_tag['href']\n",
      "                dataset_description = dataset_description_tag.get_text()\n",
      "                \n",
      "                # Check if \"Dublin Bikes\" is mentioned in the title or description\n",
      "                if \"Dublin Bikes\" in dataset_title or \"Dublin Bikes\" in dataset_description:\n",
      "                    datasets.append({'title': dataset_title, 'link': dataset_link, 'description': dataset_description})\n",
      "            else:\n",
      "                print(\"Some elements were missing on this item, skipping...\")\n",
      "        \n",
      "        if datasets:\n",
      "            df_datasets = pd.DataFrame(datasets)\n",
      "            print(df_datasets)\n",
      "            df_datasets.to_csv('dublin_bikes_datasets.csv', index=False)\n",
      "            print(\"Mentions of Dublin Bikes saved to dublin_bikes_datasets.csv\")\n",
      "        else:\n",
      "            print(\"No mentions of Dublin Bikes found in datasets.\")\n",
      "    else:\n",
      "        print(\"Failed to retrieve content from the website.\")\n",
      "\n",
      "scrape_datagov_for_dublin_bikes()\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 47:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 48:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 49:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 50:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 51:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 52:\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Code Cell 53:\n",
      "\n",
      "import tweepy\n",
      "\n",
      "# Your Twitter API credentials\n",
      "api_key = 'hMa4pEOu5d6MmpyVJe2XbdB8Q'\n",
      "api_secret_key = 'ihrPpaNz7D9TI4JwV0uOnzHuJcNNNwEpjsKP08nElkfwiqJJv3'\n",
      "access_token = '1823715008435642369-SQed9zThGNRPzql3YKm4Af5F45SI46'\n",
      "access_token_secret = 'Oo6mlRa7aWvJm4nDVZv6ciKVeR6exiXXdXK3y1MZnFlA3'\n",
      "\n",
      "# Set up OAuth 1.0a authentication\n",
      "auth = tweepy.OAuth1UserHandler(api_key, api_secret_key, access_token, access_token_secret)\n",
      "api = tweepy.API(auth)\n",
      "\n",
      "# Test: Fetch the authenticated user's timeline\n",
      "timeline = api.home_timeline(count=5)\n",
      "for tweet in timeline:\n",
      "    print(f\"{tweet.user.name} said {tweet.text}\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "All code cells have been copied to the clipboard.\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import pyperclip\n",
    "\n",
    "# Path to your Jupyter Notebook file\n",
    "notebook_path = '500SentimetAnalysis.ipynb'\n",
    "\n",
    "# Read the notebook file\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Extract all code cells\n",
    "code_cells = [cell['source'] for cell in notebook.cells if cell.cell_type == 'code']\n",
    "\n",
    "# Join all code cells into a single string\n",
    "code_cells_text = '\\n\\n'.join(code_cells)\n",
    "\n",
    "# Copy to clipboard\n",
    "pyperclip.copy(code_cells_text)\n",
    "\n",
    "# Display the extracted code cells (optional)\n",
    "for i, code in enumerate(code_cells):\n",
    "    print(f'Code Cell {i+1}:\\n')\n",
    "    print(code)\n",
    "    print('\\n' + '='*80 + '\\n')\n",
    "\n",
    "print(\"All code cells have been copied to the clipboard.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ea057-9674-4e78-902e-1f2802d31e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "100_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0517d-ce9d-4690-8aed-a1e6560a839a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d95b48-5ec8-4518-a17a-07e8ae3e0ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9359d-3f6f-4171-9900-252eb2481547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
